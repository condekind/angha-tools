"""
This type stub file was generated by pyright.
"""

from pandas.core.base import PandasObject
from contextlib import contextmanager
from typing import Any, Optional

"""
Collection of query wrappers / abstractions to both facilitate data
retrieval and to reduce dependency on DB-specific API.
"""
class SQLAlchemyRequired(ImportError):
    ...


class DatabaseError(IOError):
    ...


_SQLALCHEMY_INSTALLED = None
def _validate_flavor_parameter(flavor):
    """
    Checks whether a database 'flavor' was specified.
    If not None, produces FutureWarning if 'sqlite' and
    raises a ValueError if anything else.
    """
    ...

def _is_sqlalchemy_connectable(con):
    ...

def _convert_params(sql, params):
    """Convert SQL and params args to DBAPI2.0 compliant format."""
    ...

def _handle_date_column(col, utc: Optional[Any] = ..., format: Optional[Any] = ...):
    ...

def _parse_date_columns(data_frame, parse_dates):
    """
    Force non-datetime columns to be read as such.
    Supports both string formatted and integer timestamp columns.
    """
    ...

def _wrap_result(data, columns, index_col: Optional[Any] = ..., coerce_float: bool = ..., parse_dates: Optional[Any] = ...):
    """Wrap result set of query in a DataFrame."""
    ...

def execute(sql, con, cur: Optional[Any] = ..., params: Optional[Any] = ...):
    """
    Execute the given SQL query using the provided connection object.

    Parameters
    ----------
    sql : string
        SQL query to be executed.
    con : SQLAlchemy connectable(engine/connection) or sqlite3 connection
        Using SQLAlchemy makes it possible to use any DB supported by the
        library.
        If a DBAPI2 object, only sqlite3 is supported.
    cur : deprecated, cursor is obtained from connection, default: None
    params : list or tuple, optional, default: None
        List of parameters to pass to execute method.

    Returns
    -------
    Results Iterable
    """
    ...

def read_sql_table(table_name, con, schema: Optional[Any] = ..., index_col: Optional[Any] = ..., coerce_float: bool = ..., parse_dates: Optional[Any] = ..., columns: Optional[Any] = ..., chunksize: Optional[Any] = ...):
    """Read SQL database table into a DataFrame.

    Given a table name and a SQLAlchemy connectable, returns a DataFrame.
    This function does not support DBAPI connections.

    Parameters
    ----------
    table_name : string
        Name of SQL table in database.
    con : SQLAlchemy connectable (or database string URI)
        SQLite DBAPI connection mode not supported.
    schema : string, default None
        Name of SQL schema in database to query (if database flavor
        supports this). Uses default schema if None (default).
    index_col : string or list of strings, optional, default: None
        Column(s) to set as index(MultiIndex).
    coerce_float : boolean, default True
        Attempts to convert values of non-string, non-numeric objects (like
        decimal.Decimal) to floating point. Can result in loss of Precision.
    parse_dates : list or dict, default: None
        - List of column names to parse as dates.
        - Dict of ``{column_name: format string}`` where format string is
          strftime compatible in case of parsing string times or is one of
          (D, s, ns, ms, us) in case of parsing integer timestamps.
        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds
          to the keyword arguments of :func:`pandas.to_datetime`
          Especially useful with databases without native Datetime support,
          such as SQLite.
    columns : list, default: None
        List of column names to select from SQL table
    chunksize : int, default None
        If specified, returns an iterator where `chunksize` is the number of
        rows to include in each chunk.

    Returns
    -------
    DataFrame

    Notes
    -----
    Any datetime values with time zone information will be converted to UTC.

    See also
    --------
    read_sql_query : Read SQL query into a DataFrame.
    read_sql

    """
    ...

def read_sql_query(sql, con, index_col: Optional[Any] = ..., coerce_float: bool = ..., params: Optional[Any] = ..., parse_dates: Optional[Any] = ..., chunksize: Optional[Any] = ...):
    """Read SQL query into a DataFrame.

    Returns a DataFrame corresponding to the result set of the query
    string. Optionally provide an `index_col` parameter to use one of the
    columns as the index, otherwise default integer index will be used.

    Parameters
    ----------
    sql : string SQL query or SQLAlchemy Selectable (select or text object)
        SQL query to be executed.
    con : SQLAlchemy connectable(engine/connection), database string URI,
        or sqlite3 DBAPI2 connection
        Using SQLAlchemy makes it possible to use any DB supported by that
        library.
        If a DBAPI2 object, only sqlite3 is supported.
    index_col : string or list of strings, optional, default: None
        Column(s) to set as index(MultiIndex).
    coerce_float : boolean, default True
        Attempts to convert values of non-string, non-numeric objects (like
        decimal.Decimal) to floating point. Useful for SQL result sets.
    params : list, tuple or dict, optional, default: None
        List of parameters to pass to execute method.  The syntax used
        to pass parameters is database driver dependent. Check your
        database driver documentation for which of the five syntax styles,
        described in PEP 249's paramstyle, is supported.
        Eg. for psycopg2, uses %(name)s so use params={'name' : 'value'}
    parse_dates : list or dict, default: None
        - List of column names to parse as dates.
        - Dict of ``{column_name: format string}`` where format string is
          strftime compatible in case of parsing string times, or is one of
          (D, s, ns, ms, us) in case of parsing integer timestamps.
        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds
          to the keyword arguments of :func:`pandas.to_datetime`
          Especially useful with databases without native Datetime support,
          such as SQLite.
    chunksize : int, default None
        If specified, return an iterator where `chunksize` is the number of
        rows to include in each chunk.

    Returns
    -------
    DataFrame

    Notes
    -----
    Any datetime values with time zone information parsed via the `parse_dates`
    parameter will be converted to UTC.

    See also
    --------
    read_sql_table : Read SQL database table into a DataFrame.
    read_sql

    """
    ...

def read_sql(sql, con, index_col: Optional[Any] = ..., coerce_float: bool = ..., params: Optional[Any] = ..., parse_dates: Optional[Any] = ..., columns: Optional[Any] = ..., chunksize: Optional[Any] = ...):
    """
    Read SQL query or database table into a DataFrame.

    Parameters
    ----------
    sql : string or SQLAlchemy Selectable (select or text object)
        SQL query to be executed.
    con : SQLAlchemy connectable(engine/connection) or database string URI
        or DBAPI2 connection (fallback mode)
        Using SQLAlchemy makes it possible to use any DB supported by that
        library.
        If a DBAPI2 object, only sqlite3 is supported.
    index_col : string or list of strings, optional, default: None
        Column(s) to set as index(MultiIndex).
    coerce_float : boolean, default True
        Attempts to convert values of non-string, non-numeric objects (like
        decimal.Decimal) to floating point, useful for SQL result sets.
    params : list, tuple or dict, optional, default: None
        List of parameters to pass to execute method.  The syntax used
        to pass parameters is database driver dependent. Check your
        database driver documentation for which of the five syntax styles,
        described in PEP 249's paramstyle, is supported.
        Eg. for psycopg2, uses %(name)s so use params={'name' : 'value'}
    parse_dates : list or dict, default: None
        - List of column names to parse as dates.
        - Dict of ``{column_name: format string}`` where format string is
          strftime compatible in case of parsing string times, or is one of
          (D, s, ns, ms, us) in case of parsing integer timestamps.
        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds
          to the keyword arguments of :func:`pandas.to_datetime`
          Especially useful with databases without native Datetime support,
          such as SQLite.
    columns : list, default: None
        List of column names to select from SQL table (only used when reading
        a table).
    chunksize : int, default None
        If specified, return an iterator where `chunksize` is the
        number of rows to include in each chunk.

    Returns
    -------
    DataFrame

    Notes
    -----
    This function is a convenience wrapper around ``read_sql_table`` and
    ``read_sql_query`` (and for backward compatibility) and will delegate
    to the specific function depending on the provided input (database
    table name or SQL query).  The delegated function might have more specific
    notes about their functionality not listed here.

    See also
    --------
    read_sql_table : Read SQL database table into a DataFrame.
    read_sql_query : Read SQL query into a DataFrame.

    """
    ...

def to_sql(frame, name, con, flavor: Optional[Any] = ..., schema: Optional[Any] = ..., if_exists=..., index: bool = ..., index_label: Optional[Any] = ..., chunksize: Optional[Any] = ..., dtype: Optional[Any] = ...):
    """
    Write records stored in a DataFrame to a SQL database.

    Parameters
    ----------
    frame : DataFrame
    name : string
        Name of SQL table.
    con : SQLAlchemy connectable(engine/connection) or database string URI
        or sqlite3 DBAPI2 connection
        Using SQLAlchemy makes it possible to use any DB supported by that
        library.
        If a DBAPI2 object, only sqlite3 is supported.
    flavor : 'sqlite', default None
        .. deprecated:: 0.19.0
           'sqlite' is the only supported option if SQLAlchemy is not
           used.
    schema : string, default None
        Name of SQL schema in database to write to (if database flavor
        supports this). If None, use default schema (default).
    if_exists : {'fail', 'replace', 'append'}, default 'fail'
        - fail: If table exists, do nothing.
        - replace: If table exists, drop it, recreate it, and insert data.
        - append: If table exists, insert data. Create if does not exist.
    index : boolean, default True
        Write DataFrame index as a column.
    index_label : string or sequence, default None
        Column label for index column(s). If None is given (default) and
        `index` is True, then the index names are used.
        A sequence should be given if the DataFrame uses MultiIndex.
    chunksize : int, default None
        If not None, then rows will be written in batches of this size at a
        time.  If None, all rows will be written at once.
    dtype : single SQLtype or dict of column name to SQL type, default None
        Optional specifying the datatype for columns. The SQL type should
        be a SQLAlchemy type, or a string for sqlite3 fallback connection.
        If all columns are of the same type, one single value can be used.

    """
    ...

def has_table(table_name, con, flavor: Optional[Any] = ..., schema: Optional[Any] = ...):
    """
    Check if DataBase has named table.

    Parameters
    ----------
    table_name: string
        Name of SQL table.
    con: SQLAlchemy connectable(engine/connection) or sqlite3 DBAPI2 connection
        Using SQLAlchemy makes it possible to use any DB supported by that
        library.
        If a DBAPI2 object, only sqlite3 is supported.
    flavor : 'sqlite', default None
        .. deprecated:: 0.19.0
           'sqlite' is the only supported option if SQLAlchemy is not
           installed.
    schema : string, default None
        Name of SQL schema in database to write to (if database flavor supports
        this). If None, use default schema (default).

    Returns
    -------
    boolean
    """
    ...

table_exists = has_table
def _engine_builder(con):
    """
    Returns a SQLAlchemy engine from a URI (if con is a string)
    else it just return con without modifying it.
    """
    ...

def pandasSQL_builder(con, flavor: Optional[Any] = ..., schema: Optional[Any] = ..., meta: Optional[Any] = ..., is_cursor: bool = ...):
    """
    Convenience function to return the correct PandasSQL subclass based on the
    provided parameters.
    """
    ...

class SQLTable(PandasObject):
    """
    For mapping Pandas tables to SQL tables.
    Uses fact that table is reflected by SQLAlchemy to
    do better type conversions.
    Also holds various flags needed to avoid having to
    pass them between functions all the time.
    """
    def __init__(self, name, pandas_sql_engine, frame: Optional[Any] = ..., index: bool = ..., if_exists=..., prefix=..., index_label: Optional[Any] = ..., schema: Optional[Any] = ..., keys: Optional[Any] = ..., dtype: Optional[Any] = ...):
        self.name = ...
        self.pd_sql = ...
        self.prefix = ...
        self.frame = ...
        self.index = ...
        self.schema = ...
        self.if_exists = ...
        self.keys = ...
        self.dtype = ...
    
    def exists(self):
        ...
    
    def sql_schema(self):
        ...
    
    def _execute_create(self):
        self.table = ...
    
    def create(self):
        ...
    
    def insert_statement(self):
        ...
    
    def insert_data(self):
        ...
    
    def _execute_insert(self, conn, keys, data_iter):
        ...
    
    def insert(self, chunksize: Optional[Any] = ...):
        ...
    
    def _query_iterator(self, result, chunksize, columns, coerce_float: bool = ..., parse_dates: Optional[Any] = ...):
        """Return generator through chunked result set."""
        ...
    
    def read(self, coerce_float: bool = ..., parse_dates: Optional[Any] = ..., columns: Optional[Any] = ..., chunksize: Optional[Any] = ...):
        ...
    
    def _index_name(self, index, index_label):
        ...
    
    def _get_column_names_and_types(self, dtype_mapper):
        ...
    
    def _create_table_setup(self):
        ...
    
    def _harmonize_columns(self, parse_dates: Optional[Any] = ...):
        """
        Make the DataFrame's column types align with the SQL table
        column types.
        Need to work around limited NA value support. Floats are always
        fine, ints must always be floats if there are Null values.
        Booleans are hard because converting bool column with None replaces
        all Nones with false. Therefore only convert bool if there are no
        NA values.
        Datetimes should already be converted to np.datetime64 if supported,
        but here we also force conversion if required.
        """
        ...
    
    def _get_notna_col_dtype(self, col):
        """
        Infer datatype of the Series col.  In case the dtype of col is 'object'
        and it contains NA values, this infers the datatype of the not-NA
        values.  Needed for inserting typed data containing NULLs, GH8778.
        """
        ...
    
    def _sqlalchemy_type(self, col):
        ...
    
    def _get_dtype(self, sqltype):
        ...
    


class PandasSQL(PandasObject):
    """
    Subclasses Should define read_sql and to_sql.
    """
    def read_sql(self, *args, **kwargs):
        ...
    
    def to_sql(self, *args, **kwargs):
        ...
    


class SQLDatabase(PandasSQL):
    """
    This class enables conversion between DataFrame and SQL databases
    using SQLAlchemy to handle DataBase abstraction.

    Parameters
    ----------
    engine : SQLAlchemy connectable
        Connectable to connect with the database. Using SQLAlchemy makes it
        possible to use any DB supported by that library.
    schema : string, default None
        Name of SQL schema in database to write to (if database flavor
        supports this). If None, use default schema (default).
    meta : SQLAlchemy MetaData object, default None
        If provided, this MetaData object is used instead of a newly
        created. This allows to specify database flavor specific
        arguments in the MetaData object.

    """
    def __init__(self, engine, schema: Optional[Any] = ..., meta: Optional[Any] = ...):
        self.connectable = ...
        self.meta = ...
    
    @contextmanager
    def run_transaction(self):
        ...
    
    def execute(self, *args, **kwargs):
        """Simple passthrough to SQLAlchemy connectable"""
        ...
    
    def read_table(self, table_name, index_col: Optional[Any] = ..., coerce_float: bool = ..., parse_dates: Optional[Any] = ..., columns: Optional[Any] = ..., schema: Optional[Any] = ..., chunksize: Optional[Any] = ...):
        """Read SQL database table into a DataFrame.

        Parameters
        ----------
        table_name : string
            Name of SQL table in database.
        index_col : string, optional, default: None
            Column to set as index.
        coerce_float : boolean, default True
            Attempts to convert values of non-string, non-numeric objects
            (like decimal.Decimal) to floating point. This can result in
            loss of precision.
        parse_dates : list or dict, default: None
            - List of column names to parse as dates.
            - Dict of ``{column_name: format string}`` where format string is
              strftime compatible in case of parsing string times, or is one of
              (D, s, ns, ms, us) in case of parsing integer timestamps.
            - Dict of ``{column_name: arg}``, where the arg corresponds
              to the keyword arguments of :func:`pandas.to_datetime`.
              Especially useful with databases without native Datetime support,
              such as SQLite.
        columns : list, default: None
            List of column names to select from SQL table.
        schema : string, default None
            Name of SQL schema in database to query (if database flavor
            supports this).  If specified, this overwrites the default
            schema of the SQL database object.
        chunksize : int, default None
            If specified, return an iterator where `chunksize` is the number
            of rows to include in each chunk.

        Returns
        -------
        DataFrame

        See also
        --------
        pandas.read_sql_table
        SQLDatabase.read_query

        """
        ...
    
    @staticmethod
    def _query_iterator(result, chunksize, columns, index_col: Optional[Any] = ..., coerce_float: bool = ..., parse_dates: Optional[Any] = ...):
        """Return generator through chunked result set"""
        ...
    
    def read_query(self, sql, index_col: Optional[Any] = ..., coerce_float: bool = ..., parse_dates: Optional[Any] = ..., params: Optional[Any] = ..., chunksize: Optional[Any] = ...):
        """Read SQL query into a DataFrame.

        Parameters
        ----------
        sql : string
            SQL query to be executed.
        index_col : string, optional, default: None
            Column name to use as index for the returned DataFrame object.
        coerce_float : boolean, default True
            Attempt to convert values of non-string, non-numeric objects (like
            decimal.Decimal) to floating point, useful for SQL result sets.
        params : list, tuple or dict, optional, default: None
            List of parameters to pass to execute method.  The syntax used
            to pass parameters is database driver dependent. Check your
            database driver documentation for which of the five syntax styles,
            described in PEP 249's paramstyle, is supported.
            Eg. for psycopg2, uses %(name)s so use params={'name' : 'value'}
        parse_dates : list or dict, default: None
            - List of column names to parse as dates.
            - Dict of ``{column_name: format string}`` where format string is
              strftime compatible in case of parsing string times, or is one of
              (D, s, ns, ms, us) in case of parsing integer timestamps.
            - Dict of ``{column_name: arg dict}``, where the arg dict
              corresponds to the keyword arguments of
              :func:`pandas.to_datetime` Especially useful with databases
              without native Datetime support, such as SQLite.
        chunksize : int, default None
            If specified, return an iterator where `chunksize` is the number
            of rows to include in each chunk.

        Returns
        -------
        DataFrame

        See also
        --------
        read_sql_table : Read SQL database table into a DataFrame
        read_sql

        """
        ...
    
    read_sql = ...
    def to_sql(self, frame, name, if_exists=..., index: bool = ..., index_label: Optional[Any] = ..., schema: Optional[Any] = ..., chunksize: Optional[Any] = ..., dtype: Optional[Any] = ...):
        """
        Write records stored in a DataFrame to a SQL database.

        Parameters
        ----------
        frame : DataFrame
        name : string
            Name of SQL table.
        if_exists : {'fail', 'replace', 'append'}, default 'fail'
            - fail: If table exists, do nothing.
            - replace: If table exists, drop it, recreate it, and insert data.
            - append: If table exists, insert data. Create if does not exist.
        index : boolean, default True
            Write DataFrame index as a column.
        index_label : string or sequence, default None
            Column label for index column(s). If None is given (default) and
            `index` is True, then the index names are used.
            A sequence should be given if the DataFrame uses MultiIndex.
        schema : string, default None
            Name of SQL schema in database to write to (if database flavor
            supports this). If specified, this overwrites the default
            schema of the SQLDatabase object.
        chunksize : int, default None
            If not None, then rows will be written in batches of this size at a
            time.  If None, all rows will be written at once.
        dtype : single type or dict of column name to SQL type, default None
            Optional specifying the datatype for columns. The SQL type should
            be a SQLAlchemy type. If all columns are of the same type, one
            single value can be used.

        """
        ...
    
    @property
    def tables(self):
        ...
    
    def has_table(self, name, schema: Optional[Any] = ...):
        ...
    
    def get_table(self, table_name, schema: Optional[Any] = ...):
        ...
    
    def drop_table(self, table_name, schema: Optional[Any] = ...):
        ...
    
    def _create_sql_schema(self, frame, table_name, keys: Optional[Any] = ..., dtype: Optional[Any] = ...):
        ...
    


_SQL_TYPES = { 'string': 'TEXT','floating': 'REAL','integer': 'INTEGER','datetime': 'TIMESTAMP','date': 'DATE','time': 'TIME','boolean': 'INTEGER' }
def _get_unicode_name(name):
    ...

def _get_valid_sqlite_name(name):
    ...

_SAFE_NAMES_WARNING = "The spaces in these column names will not be changed. " "In pandas versions < 0.14, spaces were converted to " "underscores."
class SQLiteTable(SQLTable):
    """
    Patch the SQLTable for fallback support.
    Instead of a table variable just use the Create Table statement.
    """
    def __init__(self, *args, **kwargs):
        ...
    
    def sql_schema(self):
        ...
    
    def _execute_create(self):
        ...
    
    def insert_statement(self):
        ...
    
    def _execute_insert(self, conn, keys, data_iter):
        ...
    
    def _create_table_setup(self):
        """
        Return a list of SQL statements that creates a table reflecting the
        structure of a DataFrame.  The first entry will be a CREATE TABLE
        statement while the rest will be CREATE INDEX statements.
        """
        ...
    
    def _sql_type_name(self, col):
        ...
    


class SQLiteDatabase(PandasSQL):
    """
    Version of SQLDatabase to support SQLite connections (fallback without
    SQLAlchemy). This should only be used internally.

    Parameters
    ----------
    con : sqlite connection object

    """
    def __init__(self, con, flavor: Optional[Any] = ..., is_cursor: bool = ...):
        self.is_cursor = ...
        self.con = ...
    
    @contextmanager
    def run_transaction(self):
        ...
    
    def execute(self, *args, **kwargs):
        ...
    
    @staticmethod
    def _query_iterator(cursor, chunksize, columns, index_col: Optional[Any] = ..., coerce_float: bool = ..., parse_dates: Optional[Any] = ...):
        """Return generator through chunked result set"""
        ...
    
    def read_query(self, sql, index_col: Optional[Any] = ..., coerce_float: bool = ..., params: Optional[Any] = ..., parse_dates: Optional[Any] = ..., chunksize: Optional[Any] = ...):
        ...
    
    def _fetchall_as_list(self, cur):
        ...
    
    def to_sql(self, frame, name, if_exists=..., index: bool = ..., index_label: Optional[Any] = ..., schema: Optional[Any] = ..., chunksize: Optional[Any] = ..., dtype: Optional[Any] = ...):
        """
        Write records stored in a DataFrame to a SQL database.

        Parameters
        ----------
        frame: DataFrame
        name: string
            Name of SQL table.
        if_exists: {'fail', 'replace', 'append'}, default 'fail'
            fail: If table exists, do nothing.
            replace: If table exists, drop it, recreate it, and insert data.
            append: If table exists, insert data. Create if it does not exist.
        index : boolean, default True
            Write DataFrame index as a column
        index_label : string or sequence, default None
            Column label for index column(s). If None is given (default) and
            `index` is True, then the index names are used.
            A sequence should be given if the DataFrame uses MultiIndex.
        schema : string, default None
            Ignored parameter included for compatability with SQLAlchemy
            version of ``to_sql``.
        chunksize : int, default None
            If not None, then rows will be written in batches of this
            size at a time. If None, all rows will be written at once.
        dtype : single type or dict of column name to SQL type, default None
            Optional specifying the datatype for columns. The SQL type should
            be a string. If all columns are of the same type, one single value
            can be used.

        """
        ...
    
    def has_table(self, name, schema: Optional[Any] = ...):
        ...
    
    def get_table(self, table_name, schema: Optional[Any] = ...):
        ...
    
    def drop_table(self, name, schema: Optional[Any] = ...):
        ...
    
    def _create_sql_schema(self, frame, table_name, keys: Optional[Any] = ..., dtype: Optional[Any] = ...):
        ...
    


def get_schema(frame, name, flavor: Optional[Any] = ..., keys: Optional[Any] = ..., con: Optional[Any] = ..., dtype: Optional[Any] = ...):
    """
    Get the SQL db table schema for the given frame.

    Parameters
    ----------
    frame : DataFrame
    name : string
        name of SQL table
    keys : string or sequence, default: None
        columns to use a primary key
    con: an open SQL database connection object or a SQLAlchemy connectable
        Using SQLAlchemy makes it possible to use any DB supported by that
        library, default: None
        If a DBAPI2 object, only sqlite3 is supported.
    flavor : 'sqlite', default None
        .. deprecated:: 0.19.0
           'sqlite' is the only supported option if SQLAlchemy is not
           installed.
    dtype : dict of column name to SQL type, default None
        Optional specifying the datatype for columns. The SQL type should
        be a SQLAlchemy type, or a string for sqlite3 fallback connection.

    """
    ...

