"""
This type stub file was generated by pyright.
"""

from pandas import DataFrame, Panel, Panel4D, Series, SparseDataFrame, SparseSeries
from pandas.core.base import StringMixin
from pandas.compat import filter, u_safe as u
from pandas.core.computation.pytables import Expr
from typing import Any, Optional

"""
High level interface to PyTables for reading and writing pandas data structures
to disk
"""
_version = '0.15.2'
_default_encoding = 'UTF-8'
def _ensure_decoded(s):
    """ if we have bytes, decode them to unicode """
    ...

def _ensure_encoding(encoding):
    ...

def _ensure_str(name):
    """Ensure that an index / column name is a str (python 3) or
    unicode (python 2); otherwise they may be np.string dtype.
    Non-string dtypes are passed through unchanged.

    https://github.com/pandas-dev/pandas/issues/13492
    """
    ...

Term = Expr
def _ensure_term(where, scope_level):
    """
    ensure that the where is a Term or a list of Term
    this makes sure that we are capturing the scope of variables
    that are passed
    create the terms here with a frame_level=2 (we are 2 levels down)
    """
    ...

class PossibleDataLossError(Exception):
    ...


class ClosedFileError(Exception):
    ...


class IncompatibilityWarning(Warning):
    ...


incompatibility_doc = """
where criteria is being ignored as this version [%s] is too old (or
not-defined), read the file in and write it out to a new file to upgrade (with
the copy_to method)
"""
class AttributeConflictWarning(Warning):
    ...


attribute_conflict_doc = """
the [%s] attribute of the existing index is [%s] which conflicts with the new
[%s], resetting the attribute to None
"""
class DuplicateWarning(Warning):
    ...


duplicate_doc = """
duplicate entries in table, taking most recently appended
"""
performance_doc = """
your performance may suffer as PyTables will pickle object types that it cannot
map directly to c-types [inferred_type->%s,key->%s] [items->%s]
"""
_FORMAT_MAP = { u('f'): 'fixed',u('fixed'): 'fixed',u('t'): 'table',u('table'): 'table' }
format_deprecate_doc = """
the table keyword has been deprecated
use the format='fixed(f)|table(t)' keyword instead
  fixed(f) : specifies the Fixed format
             and is the default for put operations
  table(t) : specifies the Table format
             and is the default for append operations
"""
_TYPE_MAP = { Series: u('series'),SparseSeries: u('sparse_series'),DataFrame: u('frame'),SparseDataFrame: u('sparse_frame'),Panel: u('wide'),Panel4D: u('ndim') }
_STORER_MAP = { u('Series'): 'LegacySeriesFixed',u('DataFrame'): 'LegacyFrameFixed',u('DataMatrix'): 'LegacyFrameFixed',u('series'): 'SeriesFixed',u('sparse_series'): 'SparseSeriesFixed',u('frame'): 'FrameFixed',u('sparse_frame'): 'SparseFrameFixed',u('wide'): 'PanelFixed' }
_TABLE_MAP = { u('generic_table'): 'GenericTable',u('appendable_series'): 'AppendableSeriesTable',u('appendable_multiseries'): 'AppendableMultiSeriesTable',u('appendable_frame'): 'AppendableFrameTable',u('appendable_multiframe'): 'AppendableMultiFrameTable',u('appendable_panel'): 'AppendablePanelTable',u('appendable_ndim'): 'AppendableNDimTable',u('worm'): 'WORMTable',u('legacy_frame'): 'LegacyFrameTable',u('legacy_panel'): 'LegacyPanelTable' }
_AXES_MAP = { DataFrame: [0],Panel: [1, 2],Panel4D: [1, 2, 3] }
dropna_doc = """
: boolean
    drop ALL nan rows when appending to a table
"""
format_doc = """
: format
    default format writing format, if None, then
    put will default to 'fixed' and append will default to 'table'
"""
_table_mod = None
_table_file_open_policy_is_strict = False
def _tables():
    ...

def to_hdf(path_or_buf, key, value, mode: Optional[Any] = ..., complevel: Optional[Any] = ..., complib: Optional[Any] = ..., append: Optional[Any] = ..., **kwargs):
    """ store this object, close it if we opened it """
    ...

def read_hdf(path_or_buf, key: Optional[Any] = ..., mode=..., **kwargs):
    """ read from the store, close it if we opened it

        Retrieve pandas object stored in file, optionally based on where
        criteria

        Parameters
        ----------
        path_or_buf : path (string), buffer or path object (pathlib.Path or
            py._path.local.LocalPath) designating the file to open, or an
            already opened pd.HDFStore object

            .. versionadded:: 0.19.0 support for pathlib, py.path.

        key : group identifier in the store. Can be omitted if the HDF file
            contains a single pandas object.
        mode : string, {'r', 'r+', 'a'}, default 'r'. Mode to use when opening
            the file. Ignored if path_or_buf is a pd.HDFStore.
        where : list of Term (or convertable) objects, optional
        start : optional, integer (defaults to None), row number to start
            selection
        stop  : optional, integer (defaults to None), row number to stop
            selection
        columns : optional, a list of columns that if not None, will limit the
            return columns
        iterator : optional, boolean, return an iterator, default False
        chunksize : optional, nrows to include in iteration, return an iterator

        Returns
        -------
        The selected object

        """
    ...

def _is_metadata_of(group, parent_group):
    """Check if a given group is a metadata group for a given parent_group."""
    ...

class HDFStore(StringMixin):
    """
    dict-like IO interface for storing pandas objects in PyTables
    either Fixed or Table format.

    Parameters
    ----------
    path : string
        File path to HDF5 file
    mode : {'a', 'w', 'r', 'r+'}, default 'a'

        ``'r'``
            Read-only; no data can be modified.
        ``'w'``
            Write; a new file is created (an existing file with the same
            name would be deleted).
        ``'a'``
            Append; an existing file is opened for reading and writing,
            and if the file does not exist it is created.
        ``'r+'``
            It is similar to ``'a'``, but the file must already exist.
    complevel : int, 0-9, default None
            Specifies a compression level for data.
            A value of 0 disables compression.
    complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib'
            Specifies the compression library to be used.
            As of v0.20.2 these additional compressors for Blosc are supported
            (default if no compressor specified: 'blosc:blosclz'):
            {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',
             'blosc:zlib', 'blosc:zstd'}.
            Specifying a compression library which is not available issues
            a ValueError.
    fletcher32 : bool, default False
            If applying compression use the fletcher32 checksum

    Examples
    --------
    >>> from pandas import DataFrame
    >>> from numpy.random import randn
    >>> bar = DataFrame(randn(10, 4))
    >>> store = HDFStore('test.h5')
    >>> store['foo'] = bar   # write to HDF5
    >>> bar = store['foo']   # retrieve
    >>> store.close()
    """
    def __init__(self, path, mode: Optional[Any] = ..., complevel: Optional[Any] = ..., complib: Optional[Any] = ..., fletcher32: bool = ..., **kwargs):
        ...
    
    def __fspath__(self):
        ...
    
    @property
    def root(self):
        """ return the root node """
        ...
    
    @property
    def filename(self):
        ...
    
    def __getitem__(self, key):
        ...
    
    def __setitem__(self, key, value):
        ...
    
    def __delitem__(self, key):
        ...
    
    def __getattr__(self, name):
        """ allow attribute access to get stores """
        ...
    
    def __contains__(self, key):
        """ check for existance of this key
              can match the exact pathname or the pathnm w/o the leading '/'
              """
        ...
    
    def __len__(self):
        ...
    
    def __unicode__(self):
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, exc_type, exc_value, traceback):
        ...
    
    def keys(self):
        """
        Return a (potentially unordered) list of the keys corresponding to the
        objects stored in the HDFStore. These are ABSOLUTE path-names (e.g.
        have the leading '/'
        """
        ...
    
    def __iter__(self):
        ...
    
    def items(self):
        """
        iterate on key->group
        """
        ...
    
    iteritems = ...
    def open(self, mode=..., **kwargs):
        """
        Open the file in the specified mode

        Parameters
        ----------
        mode : {'a', 'w', 'r', 'r+'}, default 'a'
            See HDFStore docstring or tables.open_file for info about modes
        """
        ...
    
    def close(self):
        """
        Close the PyTables file handle
        """
        ...
    
    @property
    def is_open(self):
        """
        return a boolean indicating whether the file is open
        """
        ...
    
    def flush(self, fsync: bool = ...):
        """
        Force all buffered modifications to be written to disk.

        Parameters
        ----------
        fsync : bool (default False)
          call ``os.fsync()`` on the file handle to force writing to disk.

        Notes
        -----
        Without ``fsync=True``, flushing may not guarantee that the OS writes
        to disk. With fsync, the operation will block until the OS claims the
        file has been written; however, other caching layers may still
        interfere.
        """
        ...
    
    def get(self, key):
        """
        Retrieve pandas object stored in file

        Parameters
        ----------
        key : object

        Returns
        -------
        obj : type of object stored in file
        """
        ...
    
    def select(self, key, where: Optional[Any] = ..., start: Optional[Any] = ..., stop: Optional[Any] = ..., columns: Optional[Any] = ..., iterator: bool = ..., chunksize: Optional[Any] = ..., auto_close: bool = ..., **kwargs):
        """
        Retrieve pandas object stored in file, optionally based on where
        criteria

        Parameters
        ----------
        key : object
        where : list of Term (or convertable) objects, optional
        start : integer (defaults to None), row number to start selection
        stop  : integer (defaults to None), row number to stop selection
        columns : a list of columns that if not None, will limit the return
            columns
        iterator : boolean, return an iterator, default False
        chunksize : nrows to include in iteration, return an iterator
        auto_close : boolean, should automatically close the store when
            finished, default is False

        Returns
        -------
        The selected object

        """
        ...
    
    def select_as_coordinates(self, key, where: Optional[Any] = ..., start: Optional[Any] = ..., stop: Optional[Any] = ..., **kwargs):
        """
        return the selection as an Index

        Parameters
        ----------
        key : object
        where : list of Term (or convertable) objects, optional
        start : integer (defaults to None), row number to start selection
        stop  : integer (defaults to None), row number to stop selection
        """
        ...
    
    def select_column(self, key, column, **kwargs):
        """
        return a single column from the table. This is generally only useful to
        select an indexable

        Parameters
        ----------
        key : object
        column: the column of interest

        Exceptions
        ----------
        raises KeyError if the column is not found (or key is not a valid
            store)
        raises ValueError if the column can not be extracted individually (it
            is part of a data block)

        """
        ...
    
    def select_as_multiple(self, keys, where: Optional[Any] = ..., selector: Optional[Any] = ..., columns: Optional[Any] = ..., start: Optional[Any] = ..., stop: Optional[Any] = ..., iterator: bool = ..., chunksize: Optional[Any] = ..., auto_close: bool = ..., **kwargs):
        """ Retrieve pandas objects from multiple tables

        Parameters
        ----------
        keys : a list of the tables
        selector : the table to apply the where criteria (defaults to keys[0]
            if not supplied)
        columns : the columns I want back
        start : integer (defaults to None), row number to start selection
        stop  : integer (defaults to None), row number to stop selection
        iterator : boolean, return an iterator, default False
        chunksize : nrows to include in iteration, return an iterator

        Exceptions
        ----------
        raises KeyError if keys or selector is not found or keys is empty
        raises TypeError if keys is not a list or tuple
        raises ValueError if the tables are not ALL THE SAME DIMENSIONS
        """
        ...
    
    def put(self, key, value, format: Optional[Any] = ..., append: bool = ..., **kwargs):
        """
        Store object in HDFStore

        Parameters
        ----------
        key      : object
        value    : {Series, DataFrame, Panel}
        format   : 'fixed(f)|table(t)', default is 'fixed'
            fixed(f) : Fixed format
                       Fast writing/reading. Not-appendable, nor searchable
            table(t) : Table format
                       Write as a PyTables Table structure which may perform
                       worse but allow more flexible operations like searching
                       / selecting subsets of the data
        append   : boolean, default False
            This will force Table format, append the input data to the
            existing.
        data_columns : list of columns to create as data columns, or True to
            use all columns. See
            `here <http://pandas.pydata.org/pandas-docs/stable/io.html#query-via-data-columns>`__ # noqa
        encoding : default None, provide an encoding for strings
        dropna   : boolean, default False, do not write an ALL nan row to
            the store settable by the option 'io.hdf.dropna_table'
        """
        ...
    
    def remove(self, key, where: Optional[Any] = ..., start: Optional[Any] = ..., stop: Optional[Any] = ...):
        """
        Remove pandas object partially by specifying the where condition

        Parameters
        ----------
        key : string
            Node to remove or delete rows from
        where : list of Term (or convertable) objects, optional
        start : integer (defaults to None), row number to start selection
        stop  : integer (defaults to None), row number to stop selection

        Returns
        -------
        number of rows removed (or None if not a Table)

        Exceptions
        ----------
        raises KeyError if key is not a valid store

        """
        ...
    
    def append(self, key, value, format: Optional[Any] = ..., append: bool = ..., columns: Optional[Any] = ..., dropna: Optional[Any] = ..., **kwargs):
        """
        Append to Table in file. Node must already exist and be Table
        format.

        Parameters
        ----------
        key : object
        value : {Series, DataFrame, Panel, Panel4D}
        format: 'table' is the default
            table(t) : table format
                       Write as a PyTables Table structure which may perform
                       worse but allow more flexible operations like searching
                       / selecting subsets of the data
        append       : boolean, default True, append the input data to the
            existing
        data_columns :  list of columns, or True, default None
            List of columns to create as indexed data columns for on-disk
            queries, or True to use all columns. By default only the axes
            of the object are indexed. See `here
            <http://pandas.pydata.org/pandas-docs/stable/io.html#query-via-data-columns>`__.
        min_itemsize : dict of columns that specify minimum string sizes
        nan_rep      : string to use as string nan represenation
        chunksize    : size to chunk the writing
        expectedrows : expected TOTAL row size of this table
        encoding     : default None, provide an encoding for strings
        dropna       : boolean, default False, do not write an ALL nan row to
            the store settable by the option 'io.hdf.dropna_table'

        Notes
        -----
        Does *not* check if data being appended overlaps with existing
        data in the table, so be careful
        """
        ...
    
    def append_to_multiple(self, d, value, selector, data_columns: Optional[Any] = ..., axes: Optional[Any] = ..., dropna: bool = ..., **kwargs):
        """
        Append to multiple tables

        Parameters
        ----------
        d : a dict of table_name to table_columns, None is acceptable as the
            values of one node (this will get all the remaining columns)
        value : a pandas object
        selector : a string that designates the indexable table; all of its
            columns will be designed as data_columns, unless data_columns is
            passed, in which case these are used
        data_columns : list of columns to create as data columns, or True to
            use all columns
        dropna : if evaluates to True, drop rows from all tables if any single
                 row in each table has all NaN. Default False.

        Notes
        -----
        axes parameter is currently not accepted

        """
        ...
    
    def create_table_index(self, key, **kwargs):
        """ Create a pytables index on the table
        Parameters
        ----------
        key : object (the node to index)

        Exceptions
        ----------
        raises if the node is not a table

        """
        ...
    
    def groups(self):
        """return a list of all the top-level nodes (that are not themselves a
        pandas storage object)
        """
        ...
    
    def get_node(self, key):
        """ return the node with the key or None if it does not exist """
        ...
    
    def get_storer(self, key):
        """ return the storer object for a key, raise if not in the file """
        ...
    
    def copy(self, file, mode=..., propindexes: bool = ..., keys: Optional[Any] = ..., complib: Optional[Any] = ..., complevel: Optional[Any] = ..., fletcher32: bool = ..., overwrite: bool = ...):
        """ copy the existing store to a new file, upgrading in place

            Parameters
            ----------
            propindexes: restore indexes in copied file (defaults to True)
            keys       : list of keys to include in the copy (defaults to all)
            overwrite  : overwrite (remove and replace) existing nodes in the
                new store (default is True)
            mode, complib, complevel, fletcher32 same as in HDFStore.__init__

            Returns
            -------
            open file handle of the new store

        """
        ...
    
    def info(self):
        """
        print detailed information on the store

        .. versionadded:: 0.21.0
        """
        ...
    
    def _check_if_open(self):
        ...
    
    def _validate_format(self, format, kwargs):
        """ validate / deprecate formats; return the new kwargs """
        ...
    
    def _create_storer(self, group, format: Optional[Any] = ..., value: Optional[Any] = ..., append: bool = ..., **kwargs):
        """ return a suitable class to operate """
        ...
    
    def _write_to_group(self, key, value, format, index: bool = ..., append: bool = ..., complib: Optional[Any] = ..., encoding: Optional[Any] = ..., **kwargs):
        ...
    
    def _read_group(self, group, **kwargs):
        ...
    


def get_store(path, **kwargs):
    """ Backwards compatible alias for ``HDFStore``
    """
    ...

class TableIterator(object):
    """ define the iteration interface on a table

        Parameters
        ----------

        store : the reference store
        s     : the refered storer
        func  : the function to execute the query
        where : the where of the query
        nrows : the rows to iterate on
        start : the passed start value (default is None)
        stop  : the passed stop value (default is None)
        iterator : boolean, whether to use the default iterator
        chunksize : the passed chunking value (default is 50000)
        auto_close : boolean, automatically close the store at the end of
            iteration, default is False
        kwargs : the passed kwargs
        """
    def __init__(self, store, s, func, where, nrows, start: Optional[Any] = ..., stop: Optional[Any] = ..., iterator: bool = ..., chunksize: Optional[Any] = ..., auto_close: bool = ...):
        self.store = ...
        self.s = ...
        self.func = ...
        self.where = ...
        self.nrows = ...
        self.start = ...
        self.stop = ...
        self.coordinates = ...
        self.auto_close = ...
    
    def __iter__(self):
        ...
    
    def close(self):
        ...
    
    def get_result(self, coordinates: bool = ...):
        ...
    


class IndexCol(StringMixin):
    """ an index column description class

        Parameters
        ----------

        axis   : axis which I reference
        values : the ndarray like converted values
        kind   : a string description of this type
        typ    : the pytables type
        pos    : the position in the pytables

        """
    is_an_indexable = ...
    is_data_indexable = ...
    _info_fields = ...
    def __init__(self, values: Optional[Any] = ..., kind: Optional[Any] = ..., typ: Optional[Any] = ..., cname: Optional[Any] = ..., itemsize: Optional[Any] = ..., name: Optional[Any] = ..., axis: Optional[Any] = ..., kind_attr: Optional[Any] = ..., pos: Optional[Any] = ..., freq: Optional[Any] = ..., tz: Optional[Any] = ..., index_name: Optional[Any] = ..., **kwargs):
        self.values = ...
        self.kind = ...
        self.typ = ...
        self.itemsize = ...
        self.name = ...
        self.cname = ...
        self.kind_attr = ...
        self.axis = ...
        self.pos = ...
        self.freq = ...
        self.tz = ...
        self.index_name = ...
        self.table = ...
        self.meta = ...
        self.metadata = ...
    
    def set_name(self, name, kind_attr: Optional[Any] = ...):
        """ set the name of this indexer """
        self.name = ...
        self.kind_attr = ...
    
    def set_axis(self, axis):
        """ set the axis over which I index """
        self.axis = ...
    
    def set_pos(self, pos):
        """ set the position of this column in the Table """
        self.pos = ...
    
    def set_table(self, table):
        self.table = ...
    
    def __unicode__(self):
        ...
    
    def __eq__(self, other):
        """ compare 2 col items """
        ...
    
    def __ne__(self, other):
        ...
    
    @property
    def is_indexed(self):
        """ return whether I am an indexed column """
        ...
    
    def copy(self):
        ...
    
    def infer(self, handler):
        """infer this column from the table: create and return a new object"""
        ...
    
    def convert(self, values, nan_rep, encoding):
        """ set the values from this selection: take = take ownership """
        self.values = ...
    
    def take_data(self):
        """ return the values & release the memory """
        ...
    
    @property
    def attrs(self):
        ...
    
    @property
    def description(self):
        ...
    
    @property
    def col(self):
        """ return my current col description """
        ...
    
    @property
    def cvalues(self):
        """ return my cython values """
        ...
    
    def __iter__(self):
        ...
    
    def maybe_set_size(self, min_itemsize: Optional[Any] = ..., **kwargs):
        """ maybe set a string col itemsize:
               min_itemsize can be an integer or a dict with this columns name
               with an integer size """
        ...
    
    def validate(self, handler, append, **kwargs):
        ...
    
    def validate_names(self):
        ...
    
    def validate_and_set(self, handler, append, **kwargs):
        ...
    
    def validate_col(self, itemsize: Optional[Any] = ...):
        """ validate this column: return the compared against itemsize """
        ...
    
    def validate_attr(self, append):
        ...
    
    def update_info(self, info):
        """ set/update the info for this indexable with the key/value
            if there is a conflict raise/warn as needed """
        ...
    
    def set_info(self, info):
        """ set my state from the passed info """
        ...
    
    def get_attr(self):
        """ set the kind for this column """
        self.kind = ...
    
    def set_attr(self):
        """ set the kind for this column """
        ...
    
    def read_metadata(self, handler):
        """ retrieve the metadata for this columns """
        self.metadata = ...
    
    def validate_metadata(self, handler):
        """ validate that kind=category does not change the categories """
        ...
    
    def write_metadata(self, handler):
        """ set the meta data """
        ...
    


class GenericIndexCol(IndexCol):
    """ an index which is not represented in the data of the table """
    @property
    def is_indexed(self):
        ...
    
    def convert(self, values, nan_rep, encoding):
        """ set the values from this selection: take = take ownership """
        self.values = ...
    
    def get_attr(self):
        ...
    
    def set_attr(self):
        ...
    


class DataCol(IndexCol):
    """ a data holding column, by definition this is not indexable

        Parameters
        ----------

        data   : the actual data
        cname  : the column name in the table to hold the data (typically
                 values)
        meta   : a string description of the metadata
        metadata : the actual metadata
        """
    is_an_indexable = ...
    is_data_indexable = ...
    _info_fields = ...
    @classmethod
    def create_for_block(cls, i: Optional[Any] = ..., name: Optional[Any] = ..., cname: Optional[Any] = ..., version: Optional[Any] = ..., **kwargs):
        """ return a new datacol with the block i """
        ...
    
    def __init__(self, values: Optional[Any] = ..., kind: Optional[Any] = ..., typ: Optional[Any] = ..., cname: Optional[Any] = ..., data: Optional[Any] = ..., meta: Optional[Any] = ..., metadata: Optional[Any] = ..., block: Optional[Any] = ..., **kwargs):
        self.dtype = ...
        self.dtype_attr = ...
        self.meta = ...
        self.meta_attr = ...
    
    def __unicode__(self):
        ...
    
    def __eq__(self, other):
        """ compare 2 col items """
        ...
    
    def set_data(self, data, dtype: Optional[Any] = ...):
        self.data = ...
    
    def take_data(self):
        """ return the data & release the memory """
        ...
    
    def set_metadata(self, metadata):
        """ record the metadata """
        self.metadata = ...
    
    def set_kind(self):
        ...
    
    def set_atom(self, block, block_items, existing_col, min_itemsize, nan_rep, info, encoding: Optional[Any] = ..., **kwargs):
        """ create and setup my atom from the block b """
        self.values = ...
    
    def get_atom_string(self, block, itemsize):
        ...
    
    def set_atom_string(self, block, block_items, existing_col, min_itemsize, nan_rep, encoding):
        self.itemsize = ...
        self.kind = ...
        self.typ = ...
    
    def get_atom_coltype(self, kind: Optional[Any] = ...):
        """ return the PyTables column class for this column """
        ...
    
    def get_atom_data(self, block, kind: Optional[Any] = ...):
        ...
    
    def set_atom_complex(self, block):
        self.kind = ...
        self.typ = ...
    
    def set_atom_data(self, block):
        self.kind = ...
        self.typ = ...
    
    def set_atom_categorical(self, block, items, info: Optional[Any] = ..., values: Optional[Any] = ...):
        self.kind = ...
        self.dtype = ...
        self.ordered = ...
        self.typ = ...
        self.meta = ...
    
    def get_atom_datetime64(self, block):
        ...
    
    def set_atom_datetime64(self, block, values: Optional[Any] = ...):
        self.kind = ...
        self.typ = ...
    
    def set_atom_datetime64tz(self, block, info, values: Optional[Any] = ...):
        self.tz = ...
        self.kind = ...
        self.typ = ...
    
    def get_atom_timedelta64(self, block):
        ...
    
    def set_atom_timedelta64(self, block, values: Optional[Any] = ...):
        self.kind = ...
        self.typ = ...
    
    @property
    def shape(self):
        ...
    
    @property
    def cvalues(self):
        """ return my cython values """
        ...
    
    def validate_attr(self, append):
        """validate that we have the same order as the existing & same dtype"""
        ...
    
    def convert(self, values, nan_rep, encoding):
        """set the data from this selection (and convert to the correct dtype
        if we can)
        """
        ...
    
    def get_attr(self):
        """ get the data for this column """
        self.values = ...
        self.dtype = ...
        self.meta = ...
    
    def set_attr(self):
        """ set the data for this column """
        ...
    


class DataIndexableCol(DataCol):
    """ represent a data column that can be indexed """
    is_data_indexable = ...
    def validate_names(self):
        ...
    
    def get_atom_string(self, block, itemsize):
        ...
    
    def get_atom_data(self, block, kind: Optional[Any] = ...):
        ...
    
    def get_atom_datetime64(self, block):
        ...
    
    def get_atom_timedelta64(self, block):
        ...
    


class GenericDataIndexableCol(DataIndexableCol):
    """ represent a generic pytables data column """
    def get_attr(self):
        ...
    


class Fixed(StringMixin):
    """ represent an object in my store
        facilitate read/write of various types of objects
        this is an abstract base class

        Parameters
        ----------

        parent : my parent HDFStore
        group  : the group node where the table resides
        """
    pandas_kind = ...
    obj_type = ...
    ndim = ...
    is_table = ...
    def __init__(self, parent, group, encoding: Optional[Any] = ..., **kwargs):
        self.parent = ...
        self.group = ...
        self.encoding = ...
    
    @property
    def is_old_version(self):
        ...
    
    def set_version(self):
        """ compute and set our version """
        ...
    
    @property
    def pandas_type(self):
        ...
    
    @property
    def format_type(self):
        ...
    
    def __unicode__(self):
        """ return a pretty representation of myself """
        ...
    
    def set_object_info(self):
        """ set my pandas type & version """
        ...
    
    def copy(self):
        ...
    
    @property
    def storage_obj_type(self):
        ...
    
    @property
    def shape(self):
        ...
    
    @property
    def pathname(self):
        ...
    
    @property
    def _handle(self):
        ...
    
    @property
    def _filters(self):
        ...
    
    @property
    def _complevel(self):
        ...
    
    @property
    def _fletcher32(self):
        ...
    
    @property
    def _complib(self):
        ...
    
    @property
    def attrs(self):
        ...
    
    def set_attrs(self):
        """ set our object attributes """
        ...
    
    def get_attrs(self):
        """ get our object attributes """
        ...
    
    @property
    def storable(self):
        """ return my storable """
        ...
    
    @property
    def is_exists(self):
        ...
    
    @property
    def nrows(self):
        ...
    
    def validate(self, other):
        """ validate against an existing storable """
        ...
    
    def validate_version(self, where: Optional[Any] = ...):
        """ are we trying to operate on an old version? """
        ...
    
    def infer_axes(self):
        """ infer the axes of my storer
              return a boolean indicating if we have a valid storer or not """
        ...
    
    def read(self, **kwargs):
        ...
    
    def write(self, **kwargs):
        ...
    
    def delete(self, where: Optional[Any] = ..., start: Optional[Any] = ..., stop: Optional[Any] = ..., **kwargs):
        """
        support fully deleting the node in its entirety (only) - where
        specification must be None
        """
        ...
    


class GenericFixed(Fixed):
    """ a generified fixed version """
    _index_type_map = ...
    _reverse_index_map = ...
    attributes = ...
    def _class_to_alias(self, cls):
        ...
    
    def _alias_to_class(self, alias):
        ...
    
    def _get_index_factory(self, klass):
        ...
    
    def validate_read(self, kwargs):
        """
        remove table keywords from kwargs and return
        raise if any keywords are passed which are not-None
        """
        ...
    
    @property
    def is_exists(self):
        ...
    
    def set_attrs(self):
        """ set our object attributes """
        ...
    
    def get_attrs(self):
        """ retrieve our attributes """
        self.encoding = ...
    
    def write(self, obj, **kwargs):
        ...
    
    def read_array(self, key, start: Optional[Any] = ..., stop: Optional[Any] = ...):
        """ read an array for the specified node (off of group """
        ...
    
    def read_index(self, key, **kwargs):
        ...
    
    def write_index(self, key, index):
        ...
    
    def write_block_index(self, key, index):
        ...
    
    def read_block_index(self, key, **kwargs):
        ...
    
    def write_sparse_intindex(self, key, index):
        ...
    
    def read_sparse_intindex(self, key, **kwargs):
        ...
    
    def write_multi_index(self, key, index):
        ...
    
    def read_multi_index(self, key, **kwargs):
        ...
    
    def read_index_node(self, node, start: Optional[Any] = ..., stop: Optional[Any] = ...):
        ...
    
    def write_array_empty(self, key, value):
        """ write a 0-len array """
        ...
    
    def _is_empty_array(self, shape):
        """Returns true if any axis is zero length."""
        ...
    
    def write_array(self, key, value, items: Optional[Any] = ...):
        ...
    


class LegacyFixed(GenericFixed):
    def read_index_legacy(self, key, start: Optional[Any] = ..., stop: Optional[Any] = ...):
        ...
    


class LegacySeriesFixed(LegacyFixed):
    def read(self, **kwargs):
        ...
    


class LegacyFrameFixed(LegacyFixed):
    def read(self, **kwargs):
        ...
    


class SeriesFixed(GenericFixed):
    pandas_kind = ...
    attributes = ...
    @property
    def shape(self):
        ...
    
    def read(self, **kwargs):
        ...
    
    def write(self, obj, **kwargs):
        ...
    


class SparseFixed(GenericFixed):
    def validate_read(self, kwargs):
        """
        we don't support start, stop kwds in Sparse
        """
        ...
    


class SparseSeriesFixed(SparseFixed):
    pandas_kind = ...
    attributes = ...
    def read(self, **kwargs):
        ...
    
    def write(self, obj, **kwargs):
        ...
    


class SparseFrameFixed(SparseFixed):
    pandas_kind = ...
    attributes = ...
    def read(self, **kwargs):
        ...
    
    def write(self, obj, **kwargs):
        """ write it as a collection of individual sparse series """
        ...
    


class BlockManagerFixed(GenericFixed):
    attributes = ...
    is_shape_reversed = ...
    @property
    def shape(self):
        ...
    
    def read(self, start: Optional[Any] = ..., stop: Optional[Any] = ..., **kwargs):
        ...
    
    def write(self, obj, **kwargs):
        ...
    


class FrameFixed(BlockManagerFixed):
    pandas_kind = ...
    obj_type = ...


class PanelFixed(BlockManagerFixed):
    pandas_kind = ...
    obj_type = ...
    is_shape_reversed = ...
    def write(self, obj, **kwargs):
        ...
    


class Table(Fixed):
    """ represent a table:
          facilitate read/write of various types of tables

        Attrs in Table Node
        -------------------
        These are attributes that are store in the main table node, they are
        necessary to recreate these tables when read back in.

        index_axes    : a list of tuples of the (original indexing axis and
            index column)
        non_index_axes: a list of tuples of the (original index axis and
            columns on a non-indexing axis)
        values_axes   : a list of the columns which comprise the data of this
            table
        data_columns  : a list of the columns that we are allowing indexing
            (these become single columns in values_axes), or True to force all
            columns
        nan_rep       : the string to use for nan representations for string
            objects
        levels        : the names of levels
        metadata      : the names of the metadata columns

        """
    pandas_kind = ...
    table_type = ...
    levels = ...
    is_table = ...
    is_shape_reversed = ...
    def __init__(self, *args, **kwargs):
        self.index_axes = ...
        self.non_index_axes = ...
        self.values_axes = ...
        self.data_columns = ...
        self.metadata = ...
        self.info = ...
        self.nan_rep = ...
        self.selection = ...
    
    @property
    def table_type_short(self):
        ...
    
    @property
    def format_type(self):
        ...
    
    def __unicode__(self):
        """ return a pretty representatgion of myself """
        ...
    
    def __getitem__(self, c):
        """ return the axis for c """
        ...
    
    def validate(self, other):
        """ validate against an existing table """
        ...
    
    @property
    def is_multi_index(self):
        """the levels attribute is 1 or a list in the case of a multi-index"""
        ...
    
    def validate_metadata(self, existing):
        """ create / validate metadata """
        self.metadata = ...
    
    def validate_multiindex(self, obj):
        """validate that we can store the multi-index; reset and return the
        new object
        """
        ...
    
    @property
    def nrows_expected(self):
        """ based on our axes, compute the expected nrows """
        ...
    
    @property
    def is_exists(self):
        """ has this table been created """
        ...
    
    @property
    def storable(self):
        ...
    
    @property
    def table(self):
        """ return the table group (this is my storable) """
        ...
    
    @property
    def dtype(self):
        ...
    
    @property
    def description(self):
        ...
    
    @property
    def axes(self):
        ...
    
    @property
    def ncols(self):
        """ the number of total columns in the values axes """
        ...
    
    @property
    def is_transposed(self):
        ...
    
    @property
    def data_orientation(self):
        """return a tuple of my permutated axes, non_indexable at the front"""
        ...
    
    def queryables(self):
        """ return a dict of the kinds allowable columns for this object """
        ...
    
    def index_cols(self):
        """ return a list of my index cols """
        ...
    
    def values_cols(self):
        """ return a list of my values cols """
        ...
    
    def _get_metadata_path(self, key):
        """ return the metadata pathname for this key """
        ...
    
    def write_metadata(self, key, values):
        """
        write out a meta data array to the key as a fixed-format Series

        Parameters
        ----------
        key : string
        values : ndarray

        """
        ...
    
    def read_metadata(self, key):
        """ return the meta data array for this key """
        ...
    
    def set_info(self):
        """ update our table index info """
        ...
    
    def set_attrs(self):
        """ set our table type & indexables """
        ...
    
    def get_attrs(self):
        """ retrieve our attributes """
        self.non_index_axes = ...
        self.data_columns = ...
        self.info = ...
        self.nan_rep = ...
        self.encoding = ...
        self.levels = ...
        self.index_axes = ...
        self.values_axes = ...
        self.metadata = ...
    
    def validate_version(self, where: Optional[Any] = ...):
        """ are we trying to operate on an old version? """
        ...
    
    def validate_min_itemsize(self, min_itemsize):
        """validate the min_itemisze doesn't contain items that are not in the
        axes this needs data_columns to be defined
        """
        ...
    
    @property
    def indexables(self):
        """ create/cache the indexables if they don't exist """
        ...
    
    def create_index(self, columns: Optional[Any] = ..., optlevel: Optional[Any] = ..., kind: Optional[Any] = ...):
        """
        Create a pytables index on the specified columns
          note: cannot index Time64Col() or ComplexCol currently;
          PyTables must be >= 3.0

        Parameters
        ----------
        columns : False (don't create an index), True (create all columns
            index), None or list_like (the indexers to index)
        optlevel: optimization level (defaults to 6)
        kind    : kind of index (defaults to 'medium')

        Exceptions
        ----------
        raises if the node is not a table

        """
        ...
    
    def read_axes(self, where, **kwargs):
        """create and return the axes sniffed from the table: return boolean
        for success
        """
        self.selection = ...
    
    def get_object(self, obj):
        """ return the data for this obj """
        ...
    
    def validate_data_columns(self, data_columns, min_itemsize):
        """take the input data_columns and min_itemize and create a data
        columns spec
        """
        ...
    
    def create_axes(self, axes, obj, validate: bool = ..., nan_rep: Optional[Any] = ..., data_columns: Optional[Any] = ..., min_itemsize: Optional[Any] = ..., **kwargs):
        """ create and return the axes
        leagcy tables create an indexable column, indexable index,
        non-indexable fields

            Parameters:
            -----------
            axes: a list of the axes in order to create (names or numbers of
                the axes)
            obj : the object to create axes on
            validate: validate the obj against an existing object already
                written
            min_itemsize: a dict of the min size for a column in bytes
            nan_rep : a values to use for string column nan_rep
            encoding : the encoding for string values
            data_columns : a list of columns that we want to create separate to
                allow indexing (or True will force all columns)

        """
        self.non_index_axes = ...
        self.data_columns = ...
        self.nan_rep = ...
        self.index_axes = ...
        self.values_axes = ...
    
    def process_axes(self, obj, columns: Optional[Any] = ...):
        """ process axes filters """
        ...
    
    def create_description(self, complib: Optional[Any] = ..., complevel: Optional[Any] = ..., fletcher32: bool = ..., expectedrows: Optional[Any] = ...):
        """ create the description of the table from the axes & values """
        ...
    
    def read_coordinates(self, where: Optional[Any] = ..., start: Optional[Any] = ..., stop: Optional[Any] = ..., **kwargs):
        """select coordinates (row numbers) from a table; return the
        coordinates object
        """
        self.selection = ...
    
    def read_column(self, column, where: Optional[Any] = ..., start: Optional[Any] = ..., stop: Optional[Any] = ..., **kwargs):
        """return a single column from the table, generally only indexables
        are interesting
        """
        ...
    


class WORMTable(Table):
    """ a write-once read-many table: this format DOES NOT ALLOW appending to a
         table. writing is a one-time operation the data are stored in a format
         that allows for searching the data on disk
         """
    table_type = ...
    def read(self, **kwargs):
        """ read the indicies and the indexing array, calculate offset rows and
        return """
        ...
    
    def write(self, **kwargs):
        """ write in a format that we can search later on (but cannot append
               to): write out the indicies and the values using _write_array
               (e.g. a CArray) create an indexing table so that we can search
        """
        ...
    


class LegacyTable(Table):
    """ an appendable table: allow append/query/delete operations to a
          (possibily) already existing appendable table this table ALLOWS
          append (but doesn't require them), and stores the data in a format
          that can be easily searched

    """
    _indexables = ...
    table_type = ...
    ndim = ...
    def write(self, **kwargs):
        ...
    
    def read(self, where: Optional[Any] = ..., columns: Optional[Any] = ..., **kwargs):
        """we have n indexable columns, with an arbitrary number of data
        axes
        """
        ...
    


class LegacyFrameTable(LegacyTable):
    """ support the legacy frame table """
    pandas_kind = ...
    table_type = ...
    obj_type = ...
    def read(self, *args, **kwargs):
        ...
    


class LegacyPanelTable(LegacyTable):
    """ support the legacy panel table """
    table_type = ...
    obj_type = ...


class AppendableTable(LegacyTable):
    """ suppor the new appendable table formats """
    _indexables = ...
    table_type = ...
    def write(self, obj, axes: Optional[Any] = ..., append: bool = ..., complib: Optional[Any] = ..., complevel: Optional[Any] = ..., fletcher32: Optional[Any] = ..., min_itemsize: Optional[Any] = ..., chunksize: Optional[Any] = ..., expectedrows: Optional[Any] = ..., dropna: bool = ..., **kwargs):
        ...
    
    def write_data(self, chunksize, dropna: bool = ...):
        """ we form the data into a 2-d including indexes,values,mask
            write chunk-by-chunk """
        ...
    
    def write_data_chunk(self, rows, indexes, mask, values):
        """
        Parameters
        ----------
        rows : an empty memory space where we are putting the chunk
        indexes : an array of the indexes
        mask : an array of the masks
        values : an array of the values
        """
        ...
    
    def delete(self, where: Optional[Any] = ..., start: Optional[Any] = ..., stop: Optional[Any] = ..., **kwargs):
        self.selection = ...
    


class AppendableFrameTable(AppendableTable):
    """ suppor the new appendable table formats """
    pandas_kind = ...
    table_type = ...
    ndim = ...
    obj_type = ...
    @property
    def is_transposed(self):
        ...
    
    def get_object(self, obj):
        """ these are written transposed """
        ...
    
    def read(self, where: Optional[Any] = ..., columns: Optional[Any] = ..., **kwargs):
        ...
    


class AppendableSeriesTable(AppendableFrameTable):
    """ support the new appendable table formats """
    pandas_kind = ...
    table_type = ...
    ndim = ...
    obj_type = ...
    storage_obj_type = ...
    @property
    def is_transposed(self):
        ...
    
    def get_object(self, obj):
        ...
    
    def write(self, obj, data_columns: Optional[Any] = ..., **kwargs):
        """ we are going to write this as a frame table """
        ...
    
    def read(self, columns: Optional[Any] = ..., **kwargs):
        ...
    


class AppendableMultiSeriesTable(AppendableSeriesTable):
    """ support the new appendable table formats """
    pandas_kind = ...
    table_type = ...
    def write(self, obj, **kwargs):
        """ we are going to write this as a frame table """
        ...
    


class GenericTable(AppendableFrameTable):
    """ a table that read/writes the generic pytables table format """
    pandas_kind = ...
    table_type = ...
    ndim = ...
    obj_type = ...
    @property
    def pandas_type(self):
        ...
    
    @property
    def storable(self):
        ...
    
    def get_attrs(self):
        """ retrieve our attributes """
        self.non_index_axes = ...
        self.nan_rep = ...
        self.levels = ...
        self.index_axes = ...
        self.values_axes = ...
        self.data_columns = ...
    
    @property
    def indexables(self):
        """ create the indexables from the table description """
        ...
    
    def write(self, **kwargs):
        ...
    


class AppendableMultiFrameTable(AppendableFrameTable):
    """ a frame with a multi-index """
    table_type = ...
    obj_type = ...
    ndim = ...
    _re_levels = ...
    @property
    def table_type_short(self):
        ...
    
    def write(self, obj, data_columns: Optional[Any] = ..., **kwargs):
        ...
    
    def read(self, **kwargs):
        ...
    


class AppendablePanelTable(AppendableTable):
    """ suppor the new appendable table formats """
    table_type = ...
    ndim = ...
    obj_type = ...
    def get_object(self, obj):
        """ these are written transposed """
        ...
    
    @property
    def is_transposed(self):
        ...
    


class AppendableNDimTable(AppendablePanelTable):
    """ suppor the new appendable table formats """
    table_type = ...
    ndim = ...
    obj_type = ...


def _reindex_axis(obj, axis, labels, other: Optional[Any] = ...):
    ...

def _get_info(info, name):
    """ get/create the info for this name """
    ...

def _get_tz(tz):
    """ for a tz-aware type, return an encoded zone """
    ...

def _set_tz(values, tz, preserve_UTC: bool = ..., coerce: bool = ...):
    """
    coerce the values to a DatetimeIndex if tz is set
    preserve the input shape if possible

    Parameters
    ----------
    values : ndarray
    tz : string/pickled tz object
    preserve_UTC : boolean,
        preserve the UTC of the result
    coerce : if we do not have a passed timezone, coerce to M8[ns] ndarray
    """
    ...

def _convert_index(index, encoding: Optional[Any] = ..., format_type: Optional[Any] = ...):
    ...

def _unconvert_index(data, kind, encoding: Optional[Any] = ...):
    ...

def _unconvert_index_legacy(data, kind, legacy: bool = ..., encoding: Optional[Any] = ...):
    ...

def _convert_string_array(data, encoding, itemsize: Optional[Any] = ...):
    """
    we take a string-like that is object dtype and coerce to a fixed size
    string type

    Parameters
    ----------
    data : a numpy array of object dtype
    encoding : None or string-encoding
    itemsize : integer, optional, defaults to the max length of the strings

    Returns
    -------
    data in a fixed-length string dtype, encoded to bytes if needed
    """
    ...

def _unconvert_string_array(data, nan_rep: Optional[Any] = ..., encoding: Optional[Any] = ...):
    """
    inverse of _convert_string_array

    Parameters
    ----------
    data : fixed length string dtyped array
    nan_rep : the storage repr of NaN, optional
    encoding : the encoding of the data, optional

    Returns
    -------
    an object array of the decoded data

    """
    ...

def _maybe_convert(values, val_kind, encoding):
    ...

def _get_converter(kind, encoding):
    ...

def _need_convert(kind):
    ...

class Selection(object):
    """
    Carries out a selection operation on a tables.Table object.

    Parameters
    ----------
    table : a Table object
    where : list of Terms (or convertable to)
    start, stop: indicies to start and/or stop selection

    """
    def __init__(self, table, where: Optional[Any] = ..., start: Optional[Any] = ..., stop: Optional[Any] = ..., **kwargs):
        self.table = ...
        self.where = ...
        self.start = ...
        self.stop = ...
        self.condition = ...
        self.filter = ...
        self.terms = ...
        self.coordinates = ...
    
    def generate(self, where):
        """ where can be a : dict,list,tuple,string """
        ...
    
    def select(self):
        """
        generate the selection
        """
        ...
    
    def select_coords(self):
        """
        generate the selection
        """
        ...
    


def timeit(key, df, fn: Optional[Any] = ..., remove: bool = ..., **kwargs):
    ...

