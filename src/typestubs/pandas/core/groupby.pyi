"""
This type stub file was generated by pyright.
"""

from pandas.core.base import PandasObject, SelectionMixin
from pandas.core.frame import DataFrame
from pandas.core.generic import _shared_docs
from pandas.core.series import Series
from pandas.util._decorators import Appender, Substitution, cache_readonly
from typing import Any, Optional

_doc_template = """

        See also
        --------
        pandas.Series.%(name)s
        pandas.DataFrame.%(name)s
        pandas.Panel.%(name)s
"""
_apply_docs = dict(template="""
    Apply function ``func``  group-wise and combine the results together.

    The function passed to ``apply`` must take a {input} as its first
    argument and return a dataframe, a series or a scalar. ``apply`` will
    then take care of combining the results back together into a single
    dataframe or series. ``apply`` is therefore a highly flexible
    grouping method.

    While ``apply`` is a very flexible method, its downside is that
    using it can be quite a bit slower than using more specific methods.
    Pandas offers a wide range of method that will be much faster
    than using ``apply`` for their specific purposes, so try to use them
    before reaching for ``apply``.

    Parameters
    ----------
    func : function
        A callable that takes a {input} as its first argument, and
        returns a dataframe, a series or a scalar. In addition the
        callable may take positional and keyword arguments
    args, kwargs : tuple and dict
        Optional positional and keyword arguments to pass to ``func``

    Returns
    -------
    applied : Series or DataFrame

    Notes
    -----
    In the current implementation ``apply`` calls func twice on the
    first group to decide whether it can take a fast or slow code
    path. This can lead to unexpected behavior if func has
    side-effects, as they will take effect twice for the first
    group.

    Examples
    --------
    {examples}

    See also
    --------
    pipe : Apply function to the full GroupBy object instead of to each
        group.
    aggregate, transform
    """, dataframe_examples="""
    >>> df = pd.DataFrame({'A': 'a a b'.split(), 'B': [1,2,3], 'C': [4,6, 5]})
    >>> g = df.groupby('A')

    From ``df`` above we can see that ``g`` has two groups, ``a``, ``b``.
    Calling ``apply`` in various ways, we can get different grouping results:

    Example 1: below the function passed to ``apply`` takes a dataframe as
    its argument and returns a dataframe. ``apply`` combines the result for
    each group together into a new dataframe:

    >>> g.apply(lambda x: x / x.sum())
              B    C
    0  0.333333  0.4
    1  0.666667  0.6
    2  1.000000  1.0

    Example 2: The function passed to ``apply`` takes a dataframe as
    its argument and returns a series.  ``apply`` combines the result for
    each group together into a new dataframe:

    >>> g.apply(lambda x: x.max() - x.min())
       B  C
    A
    a  1  2
    b  0  0

    Example 3: The function passed to ``apply`` takes a dataframe as
    its argument and returns a scalar. ``apply`` combines the result for
    each group together into a series, including setting the index as
    appropriate:

    >>> g.apply(lambda x: x.C.max() - x.B.min())
    A
    a    5
    b    2
    dtype: int64
    """, series_examples="""
    >>> ser = pd.Series([0, 1, 2], index='a a b'.split())
    >>> g = ser.groupby(ser.index)

    From ``ser`` above we can see that ``g`` has two groups, ``a``, ``b``.
    Calling ``apply`` in various ways, we can get different grouping results:

    Example 1: The function passed to ``apply`` takes a series as
    its argument and returns a series.  ``apply`` combines the result for
    each group together into a new series:

    >>> g.apply(lambda x:  x*2 if x.name == 'b' else x/2)
    0    0.0
    1    0.5
    2    4.0
    dtype: float64

    Example 2: The function passed to ``apply`` takes a series as
    its argument and returns a scalar. ``apply`` combines the result for
    each group together into a series, including setting the index as
    appropriate:

    >>> g.apply(lambda x: x.max() - x.min())
    a    1
    b    0
    dtype: int64
    """)
_transform_template = """
Call function producing a like-indexed %(klass)s on each group and
return a %(klass)s having the same indexes as the original object
filled with the transformed values

Parameters
----------
f : function
    Function to apply to each group

Notes
-----
Each group is endowed the attribute 'name' in case you need to know
which group you are working on.

The current implementation imposes three requirements on f:

* f must return a value that either has the same shape as the input
  subframe or can be broadcast to the shape of the input subframe.
  For example, f returns a scalar it will be broadcast to have the
  same shape as the input subframe.
* if this is a DataFrame, f must support application column-by-column
  in the subframe. If f also supports application to the entire subframe,
  then a fast path is used starting from the second chunk.
* f must not mutate groups. Mutation is not supported and may
  produce unexpected results.

Returns
-------
%(klass)s

See also
--------
aggregate, transform

Examples
--------

# Same shape
>>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
...                           'foo', 'bar'],
...                    'B' : ['one', 'one', 'two', 'three',
...                          'two', 'two'],
...                    'C' : [1, 5, 5, 2, 5, 5],
...                    'D' : [2.0, 5., 8., 1., 2., 9.]})
>>> grouped = df.groupby('A')
>>> grouped.transform(lambda x: (x - x.mean()) / x.std())
          C         D
0 -1.154701 -0.577350
1  0.577350  0.000000
2  0.577350  1.154701
3 -1.154701 -1.000000
4  0.577350 -0.577350
5  0.577350  1.000000

# Broadcastable
>>> grouped.transform(lambda x: x.max() - x.min())
   C    D
0  4  6.0
1  3  8.0
2  4  6.0
3  3  8.0
4  4  6.0
5  3  8.0

"""
_plotting_methods = frozenset(['plot', 'boxplot', 'hist'])
_common_apply_whitelist = frozenset(['last', 'first', 'head', 'tail', 'median', 'mean', 'sum', 'min', 'max', 'cumcount', 'ngroup', 'resample', 'rank', 'quantile', 'fillna', 'mad', 'any', 'all', 'take', 'idxmax', 'idxmin', 'shift', 'tshift', 'ffill', 'bfill', 'pct_change', 'skew', 'corr', 'cov', 'diff']) | _plotting_methods
_series_apply_whitelist = _common_apply_whitelist | 'nlargest', 'nsmallest' - 'boxplot' | frozenset(['dtype', 'unique'])
_dataframe_apply_whitelist = _common_apply_whitelist | frozenset(['dtypes', 'corrwith']) - 'boxplot'
_cython_transforms = frozenset(['cumprod', 'cumsum', 'shift', 'cummin', 'cummax'])
class Grouper(object):
    """
    A Grouper allows the user to specify a groupby instruction for a target
    object

    This specification will select a column via the key parameter, or if the
    level and/or axis parameters are given, a level of the index of the target
    object.

    These are local specifications and will override 'global' settings,
    that is the parameters axis and level which are passed to the groupby
    itself.

    Parameters
    ----------
    key : string, defaults to None
        groupby key, which selects the grouping column of the target
    level : name/number, defaults to None
        the level for the target index
    freq : string / frequency object, defaults to None
        This will groupby the specified frequency if the target selection
        (via key or level) is a datetime-like object. For full specification
        of available frequencies, please see `here
        <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`_.
    axis : number/name of the axis, defaults to 0
    sort : boolean, default to False
        whether to sort the resulting labels

    additional kwargs to control time-like groupers (when ``freq`` is passed)

    closed : closed end of interval; 'left' or 'right'
    label : interval boundary to use for labeling; 'left' or 'right'
    convention : {'start', 'end', 'e', 's'}
        If grouper is PeriodIndex
    base, loffset

    Returns
    -------
    A specification for a groupby instruction

    Examples
    --------

    Syntactic sugar for ``df.groupby('A')``

    >>> df.groupby(Grouper(key='A'))

    Specify a resample operation on the column 'date'

    >>> df.groupby(Grouper(key='date', freq='60s'))

    Specify a resample operation on the level 'date' on the columns axis
    with a frequency of 60s

    >>> df.groupby(Grouper(level='date', freq='60s', axis=1))
    """
    _attributes = ...
    def __new__(cls, *args, **kwargs):
        ...
    
    def __init__(self, key: Optional[Any] = ..., level: Optional[Any] = ..., freq: Optional[Any] = ..., axis=..., sort: bool = ...):
        self.key = ...
        self.level = ...
        self.freq = ...
        self.axis = ...
        self.sort = ...
        self.grouper = ...
        self.obj = ...
        self.indexer = ...
        self.binner = ...
    
    @property
    def ax(self):
        ...
    
    def _get_grouper(self, obj, validate: bool = ...):
        """
        Parameters
        ----------
        obj : the subject object
        validate : boolean, default True
            if True, validate the grouper

        Returns
        -------
        a tuple of binner, grouper, obj (possibly sorted)
        """
        ...
    
    def _set_grouper(self, obj, sort: bool = ...):
        """
        given an object and the specifications, setup the internal grouper
        for this particular specification

        Parameters
        ----------
        obj : the subject object
        sort : bool, default False
            whether the resulting grouper should be sorted
        """
        self.obj = ...
        self.grouper = ...
    
    @property
    def groups(self):
        ...
    
    def __repr__(self):
        ...
    


class GroupByPlot(PandasObject):
    """
    Class implementing the .plot attribute for groupby objects
    """
    def __init__(self, groupby):
        ...
    
    def __call__(self, *args, **kwargs):
        ...
    
    def __getattr__(self, name):
        ...
    


class _GroupBy(PandasObject, SelectionMixin):
    _group_selection = ...
    _apply_whitelist = ...
    def __init__(self, obj, keys: Optional[Any] = ..., axis=..., level: Optional[Any] = ..., grouper: Optional[Any] = ..., exclusions: Optional[Any] = ..., selection: Optional[Any] = ..., as_index: bool = ..., sort: bool = ..., group_keys: bool = ..., squeeze: bool = ..., **kwargs):
        self.level = ...
        self.as_index = ...
        self.keys = ...
        self.sort = ...
        self.group_keys = ...
        self.squeeze = ...
        self.mutated = ...
        self.obj = ...
        self.axis = ...
        self.grouper = ...
        self.exclusions = ...
    
    def __len__(self):
        ...
    
    def __unicode__(self):
        ...
    
    def _assure_grouper(self):
        """
        we create the grouper on instantiation
        sub-classes may have a different policy
        """
        ...
    
    @property
    def groups(self):
        """ dict {group name -> group labels} """
        ...
    
    @property
    def ngroups(self):
        ...
    
    @property
    def indices(self):
        """ dict {group name -> group indices} """
        ...
    
    def _get_indices(self, names):
        """
        safe get multiple indices, translate keys for
        datelike to underlying repr
        """
        ...
    
    def _get_index(self, name):
        """ safe get index, translate keys for datelike to underlying repr """
        ...
    
    @cache_readonly
    def _selected_obj(self):
        ...
    
    def _reset_group_selection(self):
        """
        Clear group based selection. Used for methods needing to return info on
        each group regardless of whether a group selection was previously set.
        """
        ...
    
    def _set_group_selection(self):
        """
        Create group based selection. Used when selection is not passed
        directly but instead via a grouper.
        """
        ...
    
    def _set_result_index_ordered(self, result):
        ...
    
    def _dir_additions(self):
        ...
    
    def __getattr__(self, attr):
        ...
    
    plot = ...
    def _make_wrapper(self, name):
        ...
    
    def get_group(self, name, obj: Optional[Any] = ...):
        """
        Constructs NDFrame from group with provided name

        Parameters
        ----------
        name : object
            the name of the group to get as a DataFrame
        obj : NDFrame, default None
            the NDFrame to take the DataFrame out of.  If
            it is None, the object groupby was called on will
            be used

        Returns
        -------
        group : type of obj
        """
        ...
    
    def __iter__(self):
        """
        Groupby iterator

        Returns
        -------
        Generator yielding sequence of (name, subsetted object)
        for each group
        """
        ...
    
    @Appender(_apply_docs['template'].format(input="dataframe", examples=_apply_docs['dataframe_examples']))
    def apply(self, func, *args, **kwargs):
        ...
    
    def _python_apply_general(self, f):
        ...
    
    def _iterate_slices(self):
        ...
    
    def transform(self, func, *args, **kwargs):
        ...
    
    def _cumcount_array(self, ascending: bool = ...):
        """
        Parameters
        ----------
        ascending : bool, default True
            If False, number in reverse, from length of group - 1 to 0.

        Note
        ----
        this is currently implementing sort=False
        (though the default is sort=True) for groupby in general
        """
        ...
    
    def _index_with_as_index(self, b):
        """
        Take boolean mask of index to be returned from apply, if as_index=True

        """
        ...
    
    def _try_cast(self, result, obj, numeric_only: bool = ...):
        """
        try to cast the result to our obj original type,
        we may have roundtripped thru object in the mean-time

        if numeric_only is True, then only try to cast numerics
        and not datetimelikes

        """
        ...
    
    def _cython_transform(self, how, numeric_only: bool = ...):
        ...
    
    def _cython_agg_general(self, how, alt: Optional[Any] = ..., numeric_only: bool = ..., min_count=...):
        ...
    
    def _python_agg_general(self, func, *args, **kwargs):
        ...
    
    def _wrap_applied_output(self, *args, **kwargs):
        ...
    
    def _concat_objects(self, keys, values, not_indexed_same: bool = ...):
        ...
    
    def _apply_filter(self, indices, dropna):
        ...
    


class GroupBy(_GroupBy):
    """
    Class for grouping and aggregating relational data. See aggregate,
    transform, and apply functions on this object.

    It's easiest to use obj.groupby(...) to use GroupBy, but you can also do:

    ::

        grouped = groupby(obj, ...)

    Parameters
    ----------
    obj : pandas object
    axis : int, default 0
    level : int, default None
        Level of MultiIndex
    groupings : list of Grouping objects
        Most users should ignore this
    exclusions : array-like, optional
        List of columns to exclude
    name : string
        Most users should ignore this

    Notes
    -----
    After grouping, see aggregate, apply, and transform functions. Here are
    some other brief notes about usage. When grouping by multiple groups, the
    result index will be a MultiIndex (hierarchical) by default.

    Iteration produces (key, group) tuples, i.e. chunking the data by group. So
    you can write code like:

    ::

        grouped = obj.groupby(keys, axis=axis)
        for key, group in grouped:
            # do something with the data

    Function calls on GroupBy, if not specially implemented, "dispatch" to the
    grouped data. So if you group a DataFrame and wish to invoke the std()
    method on each group, you can simply do:

    ::

        df.groupby(mapper).std()

    rather than

    ::

        df.groupby(mapper).aggregate(np.std)

    You can pass arguments to these "wrapped" functions, too.

    See the online documentation for full exposition on these topics and much
    more

    Returns
    -------
    **Attributes**
    groups : dict
        {group name -> group labels}
    len(grouped) : int
        Number of groups
    """
    _apply_whitelist = ...
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def count(self):
        """Compute count of group, excluding missing values"""
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def mean(self, *args, **kwargs):
        """
        Compute mean of groups, excluding missing values

        For multiple groupings, the result index will be a MultiIndex
        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def median(self, **kwargs):
        """
        Compute median of groups, excluding missing values

        For multiple groupings, the result index will be a MultiIndex
        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def std(self, ddof=..., *args, **kwargs):
        """
        Compute standard deviation of groups, excluding missing values

        For multiple groupings, the result index will be a MultiIndex

        Parameters
        ----------
        ddof : integer, default 1
            degrees of freedom
        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def var(self, ddof=..., *args, **kwargs):
        """
        Compute variance of groups, excluding missing values

        For multiple groupings, the result index will be a MultiIndex

        Parameters
        ----------
        ddof : integer, default 1
            degrees of freedom
        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def sem(self, ddof=...):
        """
        Compute standard error of the mean of groups, excluding missing values

        For multiple groupings, the result index will be a MultiIndex

        Parameters
        ----------
        ddof : integer, default 1
            degrees of freedom
        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def size(self):
        """Compute group sizes"""
        ...
    
    @classmethod
    def _add_numeric_operations(cls):
        """ add numeric operations to the GroupBy generically """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def ohlc(self):
        """
        Compute sum of values, excluding missing values
        For multiple groupings, the result index will be a MultiIndex
        """
        ...
    
    @Appender(DataFrame.describe.__doc__)
    def describe(self, **kwargs):
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def resample(self, rule, *args, **kwargs):
        """
        Provide resampling when using a TimeGrouper
        Return a new grouper with our resampler appended
        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def rolling(self, *args, **kwargs):
        """
        Return a rolling grouper, providing rolling
        functionaility per group

        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def expanding(self, *args, **kwargs):
        """
        Return an expanding grouper, providing expanding
        functionaility per group

        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def pad(self, limit: Optional[Any] = ...):
        """
        Forward fill the values

        Parameters
        ----------
        limit : integer, optional
            limit of how many values to fill

        See Also
        --------
        Series.fillna
        DataFrame.fillna
        """
        ...
    
    ffill = ...
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def backfill(self, limit: Optional[Any] = ...):
        """
        Backward fill the values

        Parameters
        ----------
        limit : integer, optional
            limit of how many values to fill

        See Also
        --------
        Series.fillna
        DataFrame.fillna
        """
        ...
    
    bfill = ...
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def nth(self, n, dropna: Optional[Any] = ...):
        """
        Take the nth row from each group if n is an int, or a subset of rows
        if n is a list of ints.

        If dropna, will take the nth non-null row, dropna is either
        Truthy (if a Series) or 'all', 'any' (if a DataFrame);
        this is equivalent to calling dropna(how=dropna) before the
        groupby.

        Parameters
        ----------
        n : int or list of ints
            a single nth value for the row or a list of nth values
        dropna : None or str, optional
            apply the specified dropna operation before counting which row is
            the nth row. Needs to be None, 'any' or 'all'

        Examples
        --------

        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],
        ...                    'B': [np.nan, 2, 3, 4, 5]}, columns=['A', 'B'])
        >>> g = df.groupby('A')
        >>> g.nth(0)
             B
        A
        1  NaN
        2  3.0
        >>> g.nth(1)
             B
        A
        1  2.0
        2  5.0
        >>> g.nth(-1)
             B
        A
        1  4.0
        2  5.0
        >>> g.nth([0, 1])
             B
        A
        1  NaN
        1  2.0
        2  3.0
        2  5.0

        Specifying ``dropna`` allows count ignoring NaN

        >>> g.nth(0, dropna='any')
             B
        A
        1  2.0
        2  3.0

        NaNs denote group exhausted when using dropna

        >>> g.nth(3, dropna='any')
            B
        A
        1 NaN
        2 NaN

        Specifying ``as_index=False`` in ``groupby`` keeps the original index.

        >>> df.groupby('A', as_index=False).nth(1)
           A    B
        1  1  2.0
        4  2  5.0
        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def ngroup(self, ascending: bool = ...):
        """
        Number each group from 0 to the number of groups - 1.

        This is the enumerative complement of cumcount.  Note that the
        numbers given to the groups match the order in which the groups
        would be seen when iterating over the groupby object, not the
        order they are first observed.

        .. versionadded:: 0.20.2

        Parameters
        ----------
        ascending : bool, default True
            If False, number in reverse, from number of group - 1 to 0.

        Examples
        --------

        >>> df = pd.DataFrame({"A": list("aaabba")})
        >>> df
           A
        0  a
        1  a
        2  a
        3  b
        4  b
        5  a
        >>> df.groupby('A').ngroup()
        0    0
        1    0
        2    0
        3    1
        4    1
        5    0
        dtype: int64
        >>> df.groupby('A').ngroup(ascending=False)
        0    1
        1    1
        2    1
        3    0
        4    0
        5    1
        dtype: int64
        >>> df.groupby(["A", [1,1,2,3,2,1]]).ngroup()
        0    0
        1    0
        2    1
        3    3
        4    2
        5    0
        dtype: int64

        See also
        --------
        .cumcount : Number the rows in each group.

        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def cumcount(self, ascending: bool = ...):
        """
        Number each item in each group from 0 to the length of that group - 1.

        Essentially this is equivalent to

        >>> self.apply(lambda x: Series(np.arange(len(x)), x.index))

        Parameters
        ----------
        ascending : bool, default True
            If False, number in reverse, from length of group - 1 to 0.

        Examples
        --------

        >>> df = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],
        ...                   columns=['A'])
        >>> df
           A
        0  a
        1  a
        2  a
        3  b
        4  b
        5  a
        >>> df.groupby('A').cumcount()
        0    0
        1    1
        2    2
        3    0
        4    1
        5    3
        dtype: int64
        >>> df.groupby('A').cumcount(ascending=False)
        0    3
        1    2
        2    1
        3    1
        4    0
        5    0
        dtype: int64

        See also
        --------
        .ngroup : Number the groups themselves.
        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def cumprod(self, axis=..., *args, **kwargs):
        """Cumulative product for each group"""
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def cumsum(self, axis=..., *args, **kwargs):
        """Cumulative sum for each group"""
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def cummin(self, axis=..., **kwargs):
        """Cumulative min for each group"""
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def cummax(self, axis=..., **kwargs):
        """Cumulative max for each group"""
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def shift(self, periods=..., freq: Optional[Any] = ..., axis=...):
        """
        Shift each group by periods observations

        Parameters
        ----------
        periods : integer, default 1
            number of periods to shift
        freq : frequency string
        axis : axis to shift, default 0
        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def head(self, n=...):
        """
        Returns first n rows of each group.

        Essentially equivalent to ``.apply(lambda x: x.head(n))``,
        except ignores as_index flag.

        Examples
        --------

        >>> df = DataFrame([[1, 2], [1, 4], [5, 6]],
                           columns=['A', 'B'])
        >>> df.groupby('A', as_index=False).head(1)
           A  B
        0  1  2
        2  5  6
        >>> df.groupby('A').head(1)
           A  B
        0  1  2
        2  5  6
        """
        ...
    
    @Substitution(name='groupby')
    @Appender(_doc_template)
    def tail(self, n=...):
        """
        Returns last n rows of each group

        Essentially equivalent to ``.apply(lambda x: x.tail(n))``,
        except ignores as_index flag.

        Examples
        --------

        >>> df = DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],
                           columns=['A', 'B'])
        >>> df.groupby('A').tail(1)
           A  B
        1  a  2
        3  b  2
        >>> df.groupby('A').head(1)
           A  B
        0  a  1
        2  b  1
        """
        ...
    
    def pipe(self, func, *args, **kwargs):
        """ Apply a function with arguments to this GroupBy object,

        .. versionadded:: 0.21.0

        Parameters
        ----------
        func : callable or tuple of (callable, string)
            Function to apply to this GroupBy object or, alternatively, a
            ``(callable, data_keyword)`` tuple where ``data_keyword`` is a
            string indicating the keyword of ``callable`` that expects the
            GroupBy object.
        args : iterable, optional
               positional arguments passed into ``func``.
        kwargs : dict, optional
                 a dictionary of keyword arguments passed into ``func``.

        Returns
        -------
        object : the return type of ``func``.

        Notes
        -----
        Use ``.pipe`` when chaining together functions that expect
        Series, DataFrames or GroupBy objects. Instead of writing

        >>> f(g(h(df.groupby('group')), arg1=a), arg2=b, arg3=c)

        You can write

        >>> (df
        ...    .groupby('group')
        ...    .pipe(f, arg1)
        ...    .pipe(g, arg2)
        ...    .pipe(h, arg3))

        See more `here
        <http://pandas.pydata.org/pandas-docs/stable/groupby.html#pipe>`_

        See Also
        --------
        pandas.Series.pipe : Apply a function with arguments to a series
        pandas.DataFrame.pipe: Apply a function with arguments to a dataframe
        apply : Apply function to each group instead of to the
            full GroupBy object.
        """
        ...
    


@Appender(GroupBy.__doc__)
def groupby(obj, by, **kwds):
    ...

def _get_axes(group):
    ...

def _is_indexed_like(obj, axes):
    ...

class BaseGrouper(object):
    """
    This is an internal Grouper class, which actually holds
    the generated groups

    Parameters
    ----------
    axis : int
        the axis to group
    groupings : array of grouping
        all the grouping instances to handle in this grouper
        for example for grouper list to groupby, need to pass the list
    sort : boolean, default True
        whether this grouper will give sorted result or not
    group_keys : boolean, default True
    mutated : boolean, default False
    indexer : intp array, optional
        the indexer created by Grouper
        some groupers (TimeGrouper) will sort its axis and its
        group_info is also sorted, so need the indexer to reorder

    """
    def __init__(self, axis, groupings, sort: bool = ..., group_keys: bool = ..., mutated: bool = ..., indexer: Optional[Any] = ...):
        self.axis = ...
        self.groupings = ...
        self.sort = ...
        self.group_keys = ...
        self.mutated = ...
        self.indexer = ...
    
    @property
    def shape(self):
        ...
    
    def __iter__(self):
        ...
    
    @property
    def nkeys(self):
        ...
    
    def get_iterator(self, data, axis=...):
        """
        Groupby iterator

        Returns
        -------
        Generator yielding sequence of (name, subsetted object)
        for each group
        """
        ...
    
    def _get_splitter(self, data, axis=...):
        ...
    
    def _get_group_keys(self):
        ...
    
    def apply(self, f, data, axis=...):
        ...
    
    @cache_readonly
    def indices(self):
        """ dict {group name -> group indices} """
        ...
    
    @property
    def labels(self):
        ...
    
    @property
    def levels(self):
        ...
    
    @property
    def names(self):
        ...
    
    def size(self):
        """
        Compute group sizes

        """
        ...
    
    @cache_readonly
    def _max_groupsize(self):
        """
        Compute size of largest group
        """
        ...
    
    @cache_readonly
    def groups(self):
        """ dict {group name -> group labels} """
        ...
    
    @cache_readonly
    def is_monotonic(self):
        ...
    
    @cache_readonly
    def group_info(self):
        ...
    
    @cache_readonly
    def label_info(self):
        ...
    
    def _get_compressed_labels(self):
        ...
    
    @cache_readonly
    def ngroups(self):
        ...
    
    @property
    def recons_labels(self):
        ...
    
    @cache_readonly
    def result_index(self):
        ...
    
    def get_group_levels(self):
        ...
    
    _cython_functions = ...
    _cython_arity = ...
    _name_functions = ...
    def _is_builtin_func(self, arg):
        """
        if we define an builtin function for this argument, return it,
        otherwise return the arg
        """
        ...
    
    def _get_cython_function(self, kind, how, values, is_numeric):
        ...
    
    def _cython_operation(self, kind, values, how, axis, min_count=...):
        ...
    
    def aggregate(self, values, how, axis=..., min_count=...):
        ...
    
    def transform(self, values, how, axis=...):
        ...
    
    def _aggregate(self, result, counts, values, comp_ids, agg_func, is_numeric, is_datetimelike, min_count=...):
        ...
    
    def _transform(self, result, values, comp_ids, transform_func, is_numeric, is_datetimelike):
        ...
    
    def agg_series(self, obj, func):
        ...
    
    def _aggregate_series_fast(self, obj, func):
        ...
    
    def _aggregate_series_pure_python(self, obj, func):
        ...
    


def generate_bins_generic(values, binner, closed):
    """
    Generate bin edge offsets and bin labels for one array using another array
    which has bin edge values. Both arrays must be sorted.

    Parameters
    ----------
    values : array of values
    binner : a comparable array of values representing bins into which to bin
        the first array. Note, 'values' end-points must fall within 'binner'
        end-points.
    closed : which end of bin is closed; left (default), right

    Returns
    -------
    bins : array of offsets (into 'values' argument) of bins.
        Zero and last edge are excluded in result, so for instance the first
        bin is values[0:bin[0]] and the last is values[bin[-1]:]
    """
    ...

class BinGrouper(BaseGrouper):
    """
    This is an internal Grouper class

    Parameters
    ----------
    bins : the split index of binlabels to group the item of axis
    binlabels : the label list
    filter_empty : boolean, default False
    mutated : boolean, default False
    indexer : a intp array

    Examples
    --------
    bins: [2, 4, 6, 8, 10]
    binlabels: DatetimeIndex(['2005-01-01', '2005-01-03',
        '2005-01-05', '2005-01-07', '2005-01-09'],
        dtype='datetime64[ns]', freq='2D')

    the group_info, which contains the label of each item in grouped
    axis, the index of label in label list, group number, is

    (array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4]), array([0, 1, 2, 3, 4]), 5)

    means that, the grouped axis has 10 items, can be grouped into 5
    labels, the first and second items belong to the first label, the
    third and forth items belong to the second label, and so on

    """
    def __init__(self, bins, binlabels, filter_empty: bool = ..., mutated: bool = ..., indexer: Optional[Any] = ...):
        self.bins = ...
        self.binlabels = ...
        self.mutated = ...
        self.indexer = ...
    
    @cache_readonly
    def groups(self):
        """ dict {group name -> group labels} """
        ...
    
    @property
    def nkeys(self):
        ...
    
    def get_iterator(self, data, axis=...):
        """
        Groupby iterator

        Returns
        -------
        Generator yielding sequence of (name, subsetted object)
        for each group
        """
        ...
    
    @cache_readonly
    def indices(self):
        ...
    
    @cache_readonly
    def group_info(self):
        ...
    
    @cache_readonly
    def ngroups(self):
        ...
    
    @cache_readonly
    def result_index(self):
        ...
    
    @property
    def levels(self):
        ...
    
    @property
    def names(self):
        ...
    
    @property
    def groupings(self):
        ...
    
    def agg_series(self, obj, func):
        ...
    
    _cython_functions = ...


class Grouping(object):
    """
    Holds the grouping information for a single key

    Parameters
    ----------
    index : Index
    grouper :
    obj :
    name :
    level :
    in_axis : if the Grouping is a column in self.obj and hence among
        Groupby.exclusions list

    Returns
    -------
    **Attributes**:
      * indices : dict of {group -> index_list}
      * labels : ndarray, group labels
      * ids : mapping of label -> group
      * counts : array of group counts
      * group_index : unique groups
      * groups : dict of {group -> label_list}
    """
    def __init__(self, index, grouper: Optional[Any] = ..., obj: Optional[Any] = ..., name: Optional[Any] = ..., level: Optional[Any] = ..., sort: bool = ..., in_axis: bool = ...):
        self.name = ...
        self.level = ...
        self.grouper = ...
        self.index = ...
        self.sort = ...
        self.obj = ...
        self.in_axis = ...
    
    def __repr__(self):
        ...
    
    def __iter__(self):
        ...
    
    _labels = ...
    _group_index = ...
    @property
    def ngroups(self):
        ...
    
    @cache_readonly
    def indices(self):
        ...
    
    @property
    def labels(self):
        ...
    
    @property
    def group_index(self):
        ...
    
    def _make_labels(self):
        ...
    
    @cache_readonly
    def groups(self):
        ...
    


def _get_grouper(obj, key: Optional[Any] = ..., axis=..., level: Optional[Any] = ..., sort: bool = ..., mutated: bool = ..., validate: bool = ...):
    """
    create and return a BaseGrouper, which is an internal
    mapping of how to create the grouper indexers.
    This may be composed of multiple Grouping objects, indicating
    multiple groupers

    Groupers are ultimately index mappings. They can originate as:
    index mappings, keys to columns, functions, or Groupers

    Groupers enable local references to axis,level,sort, while
    the passed in axis, level, and sort are 'global'.

    This routine tries to figure out what the passing in references
    are and then creates a Grouping for each one, combined into
    a BaseGrouper.

    If validate, then check for key/level overlaps

    """
    ...

def _is_label_like(val):
    ...

def _convert_grouper(axis, grouper):
    ...

def _whitelist_method_generator(klass, whitelist):
    """
    Yields all GroupBy member defs for DataFrame/Series names in _whitelist.

    Parameters
    ----------
    klass - class where members are defined.  Should be Series or DataFrame

    whitelist - list of names of klass methods to be constructed

    Returns
    -------
    The generator yields a sequence of strings, each suitable for exec'ing,
    that define implementations of the named methods for DataFrameGroupBy
    or SeriesGroupBy.

    Since we don't want to override methods explicitly defined in the
    base class, any such name is skipped.
    """
    ...

class SeriesGroupBy(GroupBy):
    _apply_whitelist = ...
    @property
    def _selection_name(self):
        """
        since we are a series, we by definition only have
        a single name, but may be the result of a selection or
        the name of our object
        """
        ...
    
    _agg_doc = ...
    @Appender(_apply_docs['template'].format(input='series', examples=_apply_docs['series_examples']))
    def apply(self, func, *args, **kwargs):
        ...
    
    @Appender(_agg_doc)
    @Appender(_shared_docs['aggregate'] % dict(klass='Series', versionadded=''))
    def aggregate(self, func_or_funcs, *args, **kwargs):
        ...
    
    agg = ...
    def _aggregate_multiple_funcs(self, arg, _level):
        ...
    
    def _wrap_output(self, output, index, names: Optional[Any] = ...):
        """ common agg/transform wrapping logic """
        ...
    
    def _wrap_aggregated_output(self, output, names: Optional[Any] = ...):
        ...
    
    def _wrap_transformed_output(self, output, names: Optional[Any] = ...):
        ...
    
    def _wrap_applied_output(self, keys, values, not_indexed_same: bool = ...):
        ...
    
    def _aggregate_named(self, func, *args, **kwargs):
        ...
    
    @Substitution(klass='Series', selected='A.')
    @Appender(_transform_template)
    def transform(self, func, *args, **kwargs):
        ...
    
    def _transform_fast(self, func):
        """
        fast version of transform, only applicable to
        builtin/cythonizable functions
        """
        ...
    
    def filter(self, func, dropna: bool = ..., *args, **kwargs):
        """
        Return a copy of a Series excluding elements from groups that
        do not satisfy the boolean criterion specified by func.

        Parameters
        ----------
        func : function
            To apply to each group. Should return True or False.
        dropna : Drop groups that do not pass the filter. True by default;
            if False, groups that evaluate False are filled with NaNs.

        Examples
        --------
        >>> import pandas as pd
        >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
        ...                           'foo', 'bar'],
        ...                    'B' : [1, 2, 3, 4, 5, 6],
        ...                    'C' : [2.0, 5., 8., 1., 2., 9.]})
        >>> grouped = df.groupby('A')
        >>> df.groupby('A').B.filter(lambda x: x.mean() > 3.)
        1    2
        3    4
        5    6
        Name: B, dtype: int64

        Returns
        -------
        filtered : Series
        """
        ...
    
    def nunique(self, dropna: bool = ...):
        """ Returns number of unique elements in the group """
        ...
    
    @Appender(Series.describe.__doc__)
    def describe(self, **kwargs):
        ...
    
    def value_counts(self, normalize: bool = ..., sort: bool = ..., ascending: bool = ..., bins: Optional[Any] = ..., dropna: bool = ...):
        ...
    
    def count(self):
        """ Compute count of group, excluding missing values """
        ...
    
    def _apply_to_column_groupbys(self, func):
        """ return a pass thru """
        ...
    


class NDFrameGroupBy(GroupBy):
    def _iterate_slices(self):
        ...
    
    def _cython_agg_general(self, how, alt: Optional[Any] = ..., numeric_only: bool = ..., min_count=...):
        ...
    
    def _wrap_agged_blocks(self, items, blocks):
        ...
    
    _block_agg_axis = ...
    def _cython_agg_blocks(self, how, alt: Optional[Any] = ..., numeric_only: bool = ..., min_count=...):
        ...
    
    def _get_data_to_aggregate(self):
        ...
    
    def _post_process_cython_aggregate(self, obj):
        ...
    
    def aggregate(self, arg, *args, **kwargs):
        ...
    
    agg = ...
    def _aggregate_generic(self, func, *args, **kwargs):
        ...
    
    def _wrap_aggregated_output(self, output, names: Optional[Any] = ...):
        ...
    
    def _aggregate_item_by_item(self, func, *args, **kwargs):
        ...
    
    def _decide_output_index(self, output, labels):
        ...
    
    def _wrap_applied_output(self, keys, values, not_indexed_same: bool = ...):
        ...
    
    def _transform_general(self, func, *args, **kwargs):
        ...
    
    @Substitution(klass='DataFrame', selected='')
    @Appender(_transform_template)
    def transform(self, func, *args, **kwargs):
        ...
    
    def _transform_fast(self, result, obj):
        """
        Fast transform path for aggregations
        """
        ...
    
    def _define_paths(self, func, *args, **kwargs):
        ...
    
    def _choose_path(self, fast_path, slow_path, group):
        ...
    
    def _transform_item_by_item(self, obj, wrapper):
        ...
    
    def filter(self, func, dropna: bool = ..., *args, **kwargs):
        """
        Return a copy of a DataFrame excluding elements from groups that
        do not satisfy the boolean criterion specified by func.

        Parameters
        ----------
        f : function
            Function to apply to each subframe. Should return True or False.
        dropna : Drop groups that do not pass the filter. True by default;
            if False, groups that evaluate False are filled with NaNs.

        Notes
        -----
        Each subframe is endowed the attribute 'name' in case you need to know
        which group you are working on.

        Examples
        --------
        >>> import pandas as pd
        >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
        ...                           'foo', 'bar'],
        ...                    'B' : [1, 2, 3, 4, 5, 6],
        ...                    'C' : [2.0, 5., 8., 1., 2., 9.]})
        >>> grouped = df.groupby('A')
        >>> grouped.filter(lambda x: x['B'].mean() > 3.)
             A  B    C
        1  bar  2  5.0
        3  bar  4  1.0
        5  bar  6  9.0

        Returns
        -------
        filtered : DataFrame
        """
        ...
    


class DataFrameGroupBy(NDFrameGroupBy):
    _apply_whitelist = ...
    _block_agg_axis = ...
    _agg_doc = ...
    @Appender(_agg_doc)
    @Appender(_shared_docs['aggregate'] % dict(klass='DataFrame', versionadded=''))
    def aggregate(self, arg, *args, **kwargs):
        ...
    
    agg = ...
    def _gotitem(self, key, ndim, subset: Optional[Any] = ...):
        """
        sub-classes to define
        return a sliced object

        Parameters
        ----------
        key : string / list of selections
        ndim : 1,2
            requested ndim of result
        subset : object, default None
            subset to act on
        """
        ...
    
    def _wrap_generic_output(self, result, obj):
        ...
    
    def _get_data_to_aggregate(self):
        ...
    
    def _insert_inaxis_grouper_inplace(self, result):
        ...
    
    def _wrap_aggregated_output(self, output, names: Optional[Any] = ...):
        ...
    
    def _wrap_transformed_output(self, output, names: Optional[Any] = ...):
        ...
    
    def _wrap_agged_blocks(self, items, blocks):
        ...
    
    def _reindex_output(self, result):
        """
        if we have categorical groupers, then we want to make sure that
        we have a fully reindex-output to the levels. These may have not
        participated in the groupings (e.g. may have all been
        nan groups)

        This can re-expand the output space
        """
        ...
    
    def _iterate_column_groupbys(self):
        ...
    
    def _apply_to_column_groupbys(self, func):
        ...
    
    def count(self):
        """ Compute count of group, excluding missing values """
        ...
    
    def nunique(self, dropna: bool = ...):
        """
        Return DataFrame with number of distinct observations per group for
        each column.

        .. versionadded:: 0.20.0

        Parameters
        ----------
        dropna : boolean, default True
            Don't include NaN in the counts.

        Returns
        -------
        nunique: DataFrame

        Examples
        --------
        >>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',
        ...                           'ham', 'ham'],
        ...                    'value1': [1, 5, 5, 2, 5, 5],
        ...                    'value2': list('abbaxy')})
        >>> df
             id  value1 value2
        0  spam       1      a
        1   egg       5      b
        2   egg       5      b
        3  spam       2      a
        4   ham       5      x
        5   ham       5      y

        >>> df.groupby('id').nunique()
            id  value1  value2
        id
        egg    1       1       1
        ham    1       1       2
        spam   1       2       1

        # check for rows with the same id but conflicting values
        >>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())
             id  value1 value2
        0  spam       1      a
        3  spam       2      a
        4   ham       5      x
        5   ham       5      y
        """
        ...
    
    boxplot = ...


class PanelGroupBy(NDFrameGroupBy):
    def aggregate(self, arg, *args, **kwargs):
        ...
    
    agg = ...
    def _iterate_slices(self):
        ...
    
    def aggregate(self, arg, *args, **kwargs):
        """
        Aggregate using input function or dict of {column -> function}

        Parameters
        ----------
        arg : function or dict
            Function to use for aggregating groups. If a function, must either
            work when passed a Panel or when passed to Panel.apply. If
            pass a dict, the keys must be DataFrame column names

        Returns
        -------
        aggregated : Panel
        """
        ...
    
    def _wrap_generic_output(self, result, obj):
        ...
    
    def _aggregate_item_by_item(self, func, *args, **kwargs):
        ...
    
    def _wrap_aggregated_output(self, output, names: Optional[Any] = ...):
        ...
    


class NDArrayGroupBy(GroupBy):
    ...


class DataSplitter(object):
    def __init__(self, data, labels, ngroups, axis=...):
        self.data = ...
        self.labels = ...
        self.ngroups = ...
        self.axis = ...
    
    @cache_readonly
    def slabels(self):
        ...
    
    @cache_readonly
    def sort_idx(self):
        ...
    
    def __iter__(self):
        ...
    
    def _get_sorted_data(self):
        ...
    
    def _chop(self, sdata, slice_obj):
        ...
    
    def apply(self, f):
        ...
    


class ArraySplitter(DataSplitter):
    ...


class SeriesSplitter(DataSplitter):
    def _chop(self, sdata, slice_obj):
        ...
    


class FrameSplitter(DataSplitter):
    def __init__(self, data, labels, ngroups, axis=...):
        ...
    
    def fast_apply(self, f, names):
        ...
    
    def _chop(self, sdata, slice_obj):
        ...
    


class NDFrameSplitter(DataSplitter):
    def __init__(self, data, labels, ngroups, axis=...):
        self.factory = ...
    
    def _get_sorted_data(self):
        ...
    
    def _chop(self, sdata, slice_obj):
        ...
    


def get_splitter(data, *args, **kwargs):
    ...

